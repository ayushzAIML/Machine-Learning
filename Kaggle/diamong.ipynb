{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d3f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushz/.local/lib/python3.12/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's rmse: 2376.76\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's rmse: 2374.94\n",
      "LightGBM MAE: 1373.207890510665\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Load data\n",
    "train = pd.read_csv('/home/ayushz/Kaggle/Datasets/train.csv')\n",
    "test = pd.read_csv('/home/ayushz/Kaggle/Datasets/test.csv')\n",
    "\n",
    "# Initial data inspection\n",
    "train.head()\n",
    "# train_data.info()\n",
    "# train_data.describe()\n",
    "\n",
    "# Replace 0s in dimensions with NaN and attempt to fill (Note: fillna here doesn't work in-place)\n",
    "for col in ['x','y','z']:\n",
    "    train.loc[train[col] <= 0, col] = np.nan\n",
    "    test.loc[test[col] <= 0, col] = np.nan\n",
    "    train[col].fillna(train[col].median())\n",
    "    test[col].fillna(train[col].median())\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Preprocess and encode 'train' data\n",
    "train['cut'] = train['cut'].str.strip()\n",
    "train['cut'] = le.fit_transform(train['cut'])\n",
    "train['color'] = le.fit_transform(train['color'])\n",
    "train['clarity'] = le.fit_transform(train['clarity'])\n",
    "\n",
    "# Preprocess and encode 'test' data\n",
    "test['cut'] = test['cut'].str.strip()\n",
    "test['cut'] = le.fit_transform(test['cut'])\n",
    "test['color'] = le.fit_transform(test['color'])\n",
    "test['clarity'] = le.fit_transform(test['clarity'])\n",
    "\n",
    "# Clean x, y, z (dimensions) by replacing non-positive values with the median\n",
    "for col in ['x','y','z']:\n",
    "    train[col] = train[col].apply(lambda v: v if v > 0 else train[col].median())\n",
    "    test[col]  = test[col].apply(lambda v: v if v > 0 else train[col].median())\n",
    "\n",
    "# Feature engineering\n",
    "train['volume'] = train['x'] * train['y'] * train['z']\n",
    "test['volume']  = test['x'] * test['y'] * test['z']\n",
    "\n",
    "train['carat_per_volume'] = train['carat'] / (train['volume'] + 1e-9)\n",
    "test['carat_per_volume']  = test['carat'] / (test['volume'] + 1e-9)\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = train.drop(['price','id'],axis=1)\n",
    "y = train['price']\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define model parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.9,\n",
    "    'reg_lambda': 0.0,\n",
    "    'reg_alpha': 0.2222,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 65,\n",
    "    'min_split_gain': 0.2222,\n",
    "    'min_child_samples': 55,\n",
    "    'max_depth': 3,\n",
    "    'max_bin': 250,\n",
    "    'learning_rate': 0.1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train the model with early stopping\n",
    "model_lgb = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[test_data],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "\n",
    "# Predict on the test set using the best iteration\n",
    "y_pred = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"LightGBM MAE: {mae_lgb}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Submission Generation ---\n",
    "\n",
    "# 1. Prepare the competition test data for prediction\n",
    "# Ensure it has the same columns in the same order as the training data\n",
    "X_submission = test[X_train.columns]\n",
    "\n",
    "# 2. Predict on the competition test set\n",
    "# Use the model trained earlier ('model_lgb')\n",
    "competition_predictions = model_lgb.predict(X_submission, num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "# 3. Create the submission file in the correct format\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'price': competition_predictions\n",
    "})\n",
    "\n",
    "# Optional: Ensure prices are not negative\n",
    "submission_df['price'] = submission_df['price'].clip(0)\n",
    "\n",
    "# # 4. Save the submission file\n",
    "# submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "# print(\"Submission file 'submission.csv' created successfully!\")\n",
    "# print(submission_df.head())\n",
    "\n",
    "# # --- Add this after your model is trained ---\n",
    "# import joblib\n",
    "\n",
    "# # Save the model to a file\n",
    "# joblib.dump(model_lgb, 'lgbm_model.joblib')\n",
    "\n",
    "# print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c569587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM RÂ² Score: 0.6535287243179684\n",
      "LightGBM MAE: 1373.207890510665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Predict on the test set (already done)\n",
    "y_pred = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "# Calculate RÂ² score\n",
    "r2_lgb = r2_score(y_test, y_pred)\n",
    "print(f\"LightGBM RÂ² Score: {r2_lgb}\")\n",
    "\n",
    "# Optional: Already calculating MAE\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"LightGBM MAE: {mae_lgb}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51601a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting advanced optimization to achieve RÂ² â‰¥ 0.68...\n",
      "Train shape: (20000, 11), Test shape: (30000, 10)\n",
      "Removed 1758 price outliers\n",
      "Final training data shape: (18239, 11)\n",
      "Advanced data cleaning completed!\n",
      "Advanced data cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "# Advanced Feature Engineering and Model Optimization for RÂ² >= 0.68\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import Ridge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Starting advanced optimization to achieve RÂ² â‰¥ 0.68...\")\n",
    "\n",
    "# Load fresh data\n",
    "train_fresh = pd.read_csv('/home/ayushz/Kaggle/Datasets/train.csv')\n",
    "test_fresh = pd.read_csv('/home/ayushz/Kaggle/Datasets/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_fresh.shape}, Test shape: {test_fresh.shape}\")\n",
    "\n",
    "# Advanced outlier detection using IQR method\n",
    "def remove_outliers_iqr(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Remove outliers more aggressively\n",
    "original_len = len(train_fresh)\n",
    "train_fresh = remove_outliers_iqr(train_fresh, 'price', factor=1.2)\n",
    "print(f\"Removed {original_len - len(train_fresh)} price outliers\")\n",
    "\n",
    "# Remove dimension outliers\n",
    "for dim in ['x', 'y', 'z']:\n",
    "    train_fresh = remove_outliers_iqr(train_fresh, dim, factor=2.0)\n",
    "\n",
    "print(f\"Final training data shape: {train_fresh.shape}\")\n",
    "\n",
    "# More sophisticated missing value handling\n",
    "for col in ['x','y','z']:\n",
    "    # Replace very small and zero values\n",
    "    train_fresh.loc[train_fresh[col] <= 0.01, col] = np.nan\n",
    "    test_fresh.loc[test_fresh[col] <= 0.01, col] = np.nan\n",
    "    \n",
    "    # Group-based imputation by cut and clarity\n",
    "    for cut_val in train_fresh['cut'].unique():\n",
    "        for clarity_val in train_fresh['clarity'].unique():\n",
    "            mask_train = (train_fresh['cut'] == cut_val) & (train_fresh['clarity'] == clarity_val)\n",
    "            mask_test = (test_fresh['cut'] == cut_val) & (test_fresh['clarity'] == clarity_val)\n",
    "            \n",
    "            if mask_train.sum() > 0 and train_fresh.loc[mask_train, col].notna().sum() > 0:\n",
    "                median_val = train_fresh.loc[mask_train, col].median()\n",
    "                train_fresh.loc[mask_train & train_fresh[col].isna(), col] = median_val\n",
    "                test_fresh.loc[mask_test & test_fresh[col].isna(), col] = median_val\n",
    "    \n",
    "    # Fill remaining with overall median\n",
    "    overall_median = train_fresh[col].median()\n",
    "    train_fresh[col].fillna(overall_median, inplace=True)\n",
    "    test_fresh[col].fillna(overall_median, inplace=True)\n",
    "\n",
    "# Clean categorical variables\n",
    "train_fresh['cut'] = train_fresh['cut'].str.strip()\n",
    "test_fresh['cut'] = test_fresh['cut'].str.strip()\n",
    "\n",
    "print(\"Advanced data cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2acce472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced features...\n",
      "Advanced feature engineering completed!\n",
      "Train features: 35, Test features: 34\n"
     ]
    }
   ],
   "source": [
    "# Advanced Feature Engineering with Domain Knowledge\n",
    "print(\"Creating advanced features...\")\n",
    "\n",
    "# Consistent encoding\n",
    "le_cut = LabelEncoder()\n",
    "le_color = LabelEncoder()\n",
    "le_clarity = LabelEncoder()\n",
    "\n",
    "# Fit on combined data\n",
    "all_data = pd.concat([\n",
    "    train_fresh[['cut', 'color', 'clarity']], \n",
    "    test_fresh[['cut', 'color', 'clarity']]\n",
    "])\n",
    "\n",
    "le_cut.fit(all_data['cut'])\n",
    "le_color.fit(all_data['color'])\n",
    "le_clarity.fit(all_data['clarity'])\n",
    "\n",
    "train_fresh['cut_encoded'] = le_cut.transform(train_fresh['cut'])\n",
    "train_fresh['color_encoded'] = le_color.transform(train_fresh['color'])\n",
    "train_fresh['clarity_encoded'] = le_clarity.transform(train_fresh['clarity'])\n",
    "\n",
    "test_fresh['cut_encoded'] = le_cut.transform(test_fresh['cut'])\n",
    "test_fresh['color_encoded'] = le_color.transform(test_fresh['color'])\n",
    "test_fresh['clarity_encoded'] = le_clarity.transform(test_fresh['clarity'])\n",
    "\n",
    "# Basic geometric features\n",
    "train_fresh['volume'] = train_fresh['x'] * train_fresh['y'] * train_fresh['z']\n",
    "test_fresh['volume'] = test_fresh['x'] * test_fresh['y'] * test_fresh['z']\n",
    "\n",
    "train_fresh['surface_area'] = 2 * (train_fresh['x']*train_fresh['y'] + \n",
    "                                  train_fresh['x']*train_fresh['z'] + \n",
    "                                  train_fresh['y']*train_fresh['z'])\n",
    "test_fresh['surface_area'] = 2 * (test_fresh['x']*test_fresh['y'] + \n",
    "                                 test_fresh['x']*test_fresh['z'] + \n",
    "                                 test_fresh['y']*test_fresh['z'])\n",
    "\n",
    "# Advanced ratios and transformations\n",
    "train_fresh['carat_per_volume'] = train_fresh['carat'] / (train_fresh['volume'] + 1e-8)\n",
    "test_fresh['carat_per_volume'] = test_fresh['carat'] / (test_fresh['volume'] + 1e-8)\n",
    "\n",
    "train_fresh['density'] = train_fresh['carat'] / (train_fresh['volume'] + 1e-8)\n",
    "test_fresh['density'] = test_fresh['carat'] / (test_fresh['volume'] + 1e-8)\n",
    "\n",
    "# Aspect ratios\n",
    "train_fresh['aspect_xy'] = train_fresh['x'] / (train_fresh['y'] + 1e-8)\n",
    "test_fresh['aspect_xy'] = test_fresh['x'] / (test_fresh['y'] + 1e-8)\n",
    "\n",
    "train_fresh['aspect_xz'] = train_fresh['x'] / (train_fresh['z'] + 1e-8)\n",
    "test_fresh['aspect_xz'] = test_fresh['x'] / (test_fresh['z'] + 1e-8)\n",
    "\n",
    "# Diamond quality score (domain knowledge)\n",
    "# Better cut, color, clarity = higher value\n",
    "cut_quality = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n",
    "color_quality = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}\n",
    "clarity_quality = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8}\n",
    "\n",
    "train_fresh['cut_quality'] = train_fresh['cut'].map(cut_quality).fillna(3)\n",
    "test_fresh['cut_quality'] = test_fresh['cut'].map(cut_quality).fillna(3)\n",
    "\n",
    "train_fresh['color_quality'] = train_fresh['color'].map(color_quality).fillna(4)\n",
    "test_fresh['color_quality'] = test_fresh['color'].map(color_quality).fillna(4)\n",
    "\n",
    "train_fresh['clarity_quality'] = train_fresh['clarity'].map(clarity_quality).fillna(4)\n",
    "test_fresh['clarity_quality'] = test_fresh['clarity'].map(clarity_quality).fillna(4)\n",
    "\n",
    "train_fresh['overall_quality'] = (train_fresh['cut_quality'] + \n",
    "                                 train_fresh['color_quality'] + \n",
    "                                 train_fresh['clarity_quality'])\n",
    "test_fresh['overall_quality'] = (test_fresh['cut_quality'] + \n",
    "                                test_fresh['color_quality'] + \n",
    "                                test_fresh['clarity_quality'])\n",
    "\n",
    "# Log transformations for skewed features\n",
    "train_fresh['log_carat'] = np.log1p(train_fresh['carat'])\n",
    "test_fresh['log_carat'] = np.log1p(test_fresh['carat'])\n",
    "\n",
    "train_fresh['log_volume'] = np.log1p(train_fresh['volume'])\n",
    "test_fresh['log_volume'] = np.log1p(test_fresh['volume'])\n",
    "\n",
    "# Polynomial features for key variables\n",
    "for col in ['carat', 'volume', 'overall_quality']:\n",
    "    train_fresh[f'{col}_squared'] = train_fresh[col] ** 2\n",
    "    test_fresh[f'{col}_squared'] = test_fresh[col] ** 2\n",
    "    \n",
    "    train_fresh[f'{col}_cubed'] = train_fresh[col] ** 3\n",
    "    test_fresh[f'{col}_cubed'] = test_fresh[col] ** 3\n",
    "\n",
    "# Interaction features\n",
    "train_fresh['carat_quality'] = train_fresh['carat'] * train_fresh['overall_quality']\n",
    "test_fresh['carat_quality'] = test_fresh['carat'] * test_fresh['overall_quality']\n",
    "\n",
    "train_fresh['volume_quality'] = train_fresh['volume'] * train_fresh['overall_quality']\n",
    "test_fresh['volume_quality'] = test_fresh['volume'] * test_fresh['overall_quality']\n",
    "\n",
    "# Binning features\n",
    "train_fresh['carat_bin'] = pd.cut(train_fresh['carat'], bins=10, labels=False)\n",
    "test_fresh['carat_bin'] = pd.cut(test_fresh['carat'], bins=10, labels=False)\n",
    "\n",
    "print(\"Advanced feature engineering completed!\")\n",
    "print(f\"Train features: {train_fresh.shape[1]}, Test features: {test_fresh.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21b88371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training optimized LightGBM and CatBoost models...\n",
      "Feature matrix shape: (18239, 30)\n",
      "Target shape: (18239,)\n",
      "Advanced train set: (14591, 30)\n",
      "Advanced test set: (3648, 30)\n",
      "Training ultra-optimized LightGBM...\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's rmse: 1418.22\n",
      "Ultra LightGBM - RÂ²: 0.6780, MAE: 921.25\n",
      "Training optimized CatBoost...\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/private/libs/options/bootstrap_options.cpp:16: Error: bayesian bootstrap doesn't support 'subsample' option",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 131\u001b[0m\n\u001b[1;32m     98\u001b[0m catboost_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5000\u001b[39m,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboost_from_average\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    128\u001b[0m }\n\u001b[1;32m    130\u001b[0m model_catboost \u001b[38;5;241m=\u001b[39m CatBoostRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcatboost_params)\n\u001b[0;32m--> 131\u001b[0m \u001b[43mmodel_catboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_adv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_adv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    136\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m y_pred_cb \u001b[38;5;241m=\u001b[39m model_catboost\u001b[38;5;241m.\u001b[39mpredict(X_test_adv)\n\u001b[1;32m    139\u001b[0m r2_cb \u001b[38;5;241m=\u001b[39m r2_score(y_test_adv, y_pred_cb)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/catboost/core.py:5873\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   5872\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m-> 5873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5874\u001b[0m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5875\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5876\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/catboost/core.py:2395\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, PATH_TYPES \u001b[38;5;241m+\u001b[39m (Pool,)):\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my may be None only when X is an instance of catboost.Pool or string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2395\u001b[0m train_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_train_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubgroup_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubgroup_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairs_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpairs_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2405\u001b[0m params \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2406\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/catboost/core.py:2321\u001b[0m, in \u001b[0;36mCatBoost._prepare_train_params\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[1;32m   2319\u001b[0m _check_param_types(params)\n\u001b[1;32m   2320\u001b[0m params \u001b[38;5;241m=\u001b[39m _params_type_cast(params)\n\u001b[0;32m-> 2321\u001b[0m \u001b[43m_check_train_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m_catboost.pyx:6601\u001b[0m, in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:6623\u001b[0m, in \u001b[0;36m_catboost._check_train_params\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: catboost/private/libs/options/bootstrap_options.cpp:16: Error: bayesian bootstrap doesn't support 'subsample' option"
     ]
    }
   ],
   "source": [
    "# Optimized LightGBM and CatBoost for RÂ² >= 0.68\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Training optimized LightGBM and CatBoost models...\")\n",
    "\n",
    "# Select features\n",
    "feature_cols = [\n",
    "    'carat', 'cut_encoded', 'color_encoded', 'clarity_encoded', 'depth', 'table',\n",
    "    'x', 'y', 'z', 'volume', 'surface_area', 'carat_per_volume', 'density',\n",
    "    'aspect_xy', 'aspect_xz', 'cut_quality', 'color_quality', 'clarity_quality',\n",
    "    'overall_quality', 'log_carat', 'log_volume', 'carat_squared', 'volume_squared',\n",
    "    'overall_quality_squared', 'carat_cubed', 'volume_cubed', 'overall_quality_cubed',\n",
    "    'carat_quality', 'volume_quality', 'carat_bin'\n",
    "]\n",
    "\n",
    "X_advanced = train_fresh[feature_cols]\n",
    "y_advanced = train_fresh['price']\n",
    "\n",
    "print(f\"Feature matrix shape: {X_advanced.shape}\")\n",
    "print(f\"Target shape: {y_advanced.shape}\")\n",
    "\n",
    "# Advanced train-test split with stratification\n",
    "X_train_adv, X_test_adv, y_train_adv, y_test_adv = train_test_split(\n",
    "    X_advanced, y_advanced, test_size=0.2, random_state=42, \n",
    "    stratify=pd.cut(y_advanced, bins=10, labels=False)\n",
    ")\n",
    "\n",
    "print(f\"Advanced train set: {X_train_adv.shape}\")\n",
    "print(f\"Advanced test set: {X_test_adv.shape}\")\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# 1. Ultra-optimized LightGBM with aggressive hyperparameters\n",
    "print(\"Training ultra-optimized LightGBM...\")\n",
    "\n",
    "params_lgb_ultra = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 300,          # Increased significantly\n",
    "    'max_depth': 15,            # Increased depth\n",
    "    'learning_rate': 0.01,      # Lower learning rate for more iterations\n",
    "    'feature_fraction': 0.95,   # Use more features\n",
    "    'bagging_fraction': 0.95,   # Use more data\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 5,     # Lower minimum samples\n",
    "    'min_split_gain': 0.01,     # Lower split gain threshold\n",
    "    'reg_alpha': 0.01,          # Light regularization\n",
    "    'reg_lambda': 0.01,\n",
    "    'subsample': 0.95,\n",
    "    'colsample_bytree': 0.95,\n",
    "    'max_bin': 500,             # More bins for better splits\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'extra_trees': True,        # Extra randomization\n",
    "    'force_row_wise': True,\n",
    "    'device': 'cpu',\n",
    "    'num_threads': -1\n",
    "}\n",
    "\n",
    "train_data_lgb = lgb.Dataset(X_train_adv, label=y_train_adv)\n",
    "valid_data_lgb = lgb.Dataset(X_test_adv, label=y_test_adv, reference=train_data_lgb)\n",
    "\n",
    "model_lgb_ultra = lgb.train(\n",
    "    params_lgb_ultra,\n",
    "    train_data_lgb,\n",
    "    valid_sets=[valid_data_lgb],\n",
    "    num_boost_round=8000,       # Much higher iterations\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(300),  # Higher patience\n",
    "        lgb.log_evaluation(0)     # Silent\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred_lgb = model_lgb_ultra.predict(X_test_adv, num_iteration=model_lgb_ultra.best_iteration)\n",
    "r2_lgb = r2_score(y_test_adv, y_pred_lgb)\n",
    "mae_lgb = mean_absolute_error(y_test_adv, y_pred_lgb)\n",
    "\n",
    "best_models['Ultra_LightGBM'] = {\n",
    "    'model': model_lgb_ultra,\n",
    "    'r2': r2_lgb,\n",
    "    'mae': mae_lgb,\n",
    "    'predictions': y_pred_lgb\n",
    "}\n",
    "\n",
    "print(f\"Ultra LightGBM - RÂ²: {r2_lgb:.4f}, MAE: {mae_lgb:.2f}\")\n",
    "\n",
    "# 2. Optimized CatBoost\n",
    "print(\"Training optimized CatBoost...\")\n",
    "\n",
    "# CatBoost parameters optimized for diamond price prediction\n",
    "catboost_params = {\n",
    "    'iterations': 5000,\n",
    "    'learning_rate': 0.01,\n",
    "    'depth': 12,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'border_count': 254,\n",
    "    'feature_border_type': 'GreedyLogSum',\n",
    "    'bagging_temperature': 0.2,\n",
    "    'random_strength': 0.2,\n",
    "    'od_type': 'Iter',\n",
    "    'od_wait': 300,\n",
    "    'random_seed': 42,\n",
    "    'verbose': False,\n",
    "    'thread_count': -1,\n",
    "    'task_type': 'CPU',\n",
    "    'bootstrap_type': 'Bernoulli',  # Changed from Bayesian\n",
    "    'subsample': 0.95,\n",
    "    'sampling_frequency': 'PerTreeLevel',\n",
    "    'leaf_estimation_method': 'Newton',\n",
    "    'grow_policy': 'SymmetricTree',\n",
    "    'penalties_coefficient': 1,\n",
    "    'boosting_type': 'Plain',\n",
    "    'model_shrink_rate': 0.1,\n",
    "    'model_shrink_mode': 'Constant',\n",
    "    'langevin': False,\n",
    "    'diffusion_temperature': 10000,\n",
    "    'posterior_sampling': False,\n",
    "    'boost_from_average': True\n",
    "}\n",
    "\n",
    "model_catboost = CatBoostRegressor(**catboost_params)\n",
    "model_catboost.fit(\n",
    "    X_train_adv, y_train_adv,\n",
    "    eval_set=(X_test_adv, y_test_adv),\n",
    "    verbose=False,\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "y_pred_cb = model_catboost.predict(X_test_adv)\n",
    "r2_cb = r2_score(y_test_adv, y_pred_cb)\n",
    "mae_cb = mean_absolute_error(y_test_adv, y_pred_cb)\n",
    "\n",
    "best_models['Optimized_CatBoost'] = {\n",
    "    'model': model_catboost,\n",
    "    'r2': r2_cb,\n",
    "    'mae': mae_cb,\n",
    "    'predictions': y_pred_cb\n",
    "}\n",
    "\n",
    "print(f\"Optimized CatBoost - RÂ²: {r2_cb:.4f}, MAE: {mae_cb:.2f}\")\n",
    "\n",
    "# 3. Alternative LightGBM with different approach\n",
    "print(\"Training alternative LightGBM with DART boosting...\")\n",
    "\n",
    "params_lgb_dart = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'dart',    # DART boosting\n",
    "    'num_leaves': 255,\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 10,\n",
    "    'min_split_gain': 0.02,\n",
    "    'reg_alpha': 0.05,\n",
    "    'reg_lambda': 0.05,\n",
    "    'drop_rate': 0.1,           # DART specific\n",
    "    'max_drop': 50,             # DART specific\n",
    "    'skip_drop': 0.5,           # DART specific\n",
    "    'xgboost_dart_mode': False,\n",
    "    'uniform_drop': False,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'force_row_wise': True\n",
    "}\n",
    "\n",
    "model_lgb_dart = lgb.train(\n",
    "    params_lgb_dart,\n",
    "    train_data_lgb,\n",
    "    valid_sets=[valid_data_lgb],\n",
    "    num_boost_round=4000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(200),\n",
    "        lgb.log_evaluation(0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred_dart = model_lgb_dart.predict(X_test_adv, num_iteration=model_lgb_dart.best_iteration)\n",
    "r2_dart = r2_score(y_test_adv, y_pred_dart)\n",
    "mae_dart = mean_absolute_error(y_test_adv, y_pred_dart)\n",
    "\n",
    "best_models['LightGBM_DART'] = {\n",
    "    'model': model_lgb_dart,\n",
    "    'r2': r2_dart,\n",
    "    'mae': mae_dart,\n",
    "    'predictions': y_pred_dart\n",
    "}\n",
    "\n",
    "print(f\"LightGBM DART - RÂ²: {r2_dart:.4f}, MAE: {mae_dart:.2f}\")\n",
    "\n",
    "print(\"\\\\nAll specialized models completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e857878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nContinuing with fixed CatBoost and more LightGBM variants...\n",
      "Training fixed CatBoost...\n",
      "CatBoost failed: CatBoostRegressor.__init__() got an unexpected keyword argument 'auto_class_weights'\n",
      "Continuing without CatBoost...\n",
      "Training ultra-fine-tuned LightGBM to exceed 0.68...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] Cannot change max_bin after constructed Dataset handle.\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Cannot change max_bin after constructed Dataset handle.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 86\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining ultra-fine-tuned LightGBM to exceed 0.68...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m params_lgb_ultra_fine \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.005\u001b[39m\n\u001b[1;32m     84\u001b[0m }\n\u001b[0;32m---> 86\u001b[0m model_lgb_ultra_fine \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_lgb_ultra_fine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_lgb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvalid_data_lgb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Very high iterations\u001b[39;49;00m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Very high patience\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m y_pred_ultra_fine \u001b[38;5;241m=\u001b[39m model_lgb_ultra_fine\u001b[38;5;241m.\u001b[39mpredict(X_test_adv, num_iteration\u001b[38;5;241m=\u001b[39mmodel_lgb_ultra_fine\u001b[38;5;241m.\u001b[39mbest_iteration)\n\u001b[1;32m     98\u001b[0m r2_ultra_fine \u001b[38;5;241m=\u001b[39m r2_score(y_test_adv, y_pred_ultra_fine)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/engine.py:244\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     init_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 244\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_set_predictor(predictor)\n\u001b[1;32m    246\u001b[0m is_valid_contain_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    247\u001b[0m train_data_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py:2747\u001b[0m, in \u001b[0;36mDataset._update_params\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m   2745\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_free_handle()\n\u001b[1;32m   2746\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2747\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Cannot change max_bin after constructed Dataset handle."
     ]
    }
   ],
   "source": [
    "# Continue with CatBoost (fixed) and additional LightGBM optimization\n",
    "print(\"\\\\nContinuing with fixed CatBoost and more LightGBM variants...\")\n",
    "\n",
    "# 2. Fixed CatBoost\n",
    "print(\"Training fixed CatBoost...\")\n",
    "try:\n",
    "    catboost_params_fixed = {\n",
    "        'iterations': 4000,\n",
    "        'learning_rate': 0.015,\n",
    "        'depth': 10,\n",
    "        'l2_leaf_reg': 5,\n",
    "        'border_count': 200,\n",
    "        'bagging_temperature': 0.3,\n",
    "        'random_strength': 0.3,\n",
    "        'od_type': 'Iter',\n",
    "        'od_wait': 200,\n",
    "        'random_seed': 42,\n",
    "        'verbose': False,\n",
    "        'thread_count': -1,\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'subsample': 0.9,\n",
    "        'leaf_estimation_method': 'Newton',\n",
    "        'grow_policy': 'SymmetricTree',\n",
    "        'boost_from_average': True,\n",
    "        'auto_class_weights': None\n",
    "    }\n",
    "    \n",
    "    model_catboost_fixed = CatBoostRegressor(**catboost_params_fixed)\n",
    "    model_catboost_fixed.fit(\n",
    "        X_train_adv, y_train_adv,\n",
    "        eval_set=(X_test_adv, y_test_adv),\n",
    "        verbose=False,\n",
    "        plot=False,\n",
    "        early_stopping_rounds=200\n",
    "    )\n",
    "    \n",
    "    y_pred_cb_fixed = model_catboost_fixed.predict(X_test_adv)\n",
    "    r2_cb_fixed = r2_score(y_test_adv, y_pred_cb_fixed)\n",
    "    mae_cb_fixed = mean_absolute_error(y_test_adv, y_pred_cb_fixed)\n",
    "    \n",
    "    best_models['Fixed_CatBoost'] = {\n",
    "        'model': model_catboost_fixed,\n",
    "        'r2': r2_cb_fixed,\n",
    "        'mae': mae_cb_fixed,\n",
    "        'predictions': y_pred_cb_fixed\n",
    "    }\n",
    "    \n",
    "    print(f\"Fixed CatBoost - RÂ²: {r2_cb_fixed:.4f}, MAE: {mae_cb_fixed:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CatBoost failed: {e}\")\n",
    "    print(\"Continuing without CatBoost...\")\n",
    "\n",
    "# 4. Ultra-fine-tuned LightGBM to push RÂ² above 0.68\n",
    "print(\"Training ultra-fine-tuned LightGBM to exceed 0.68...\")\n",
    "\n",
    "params_lgb_ultra_fine = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 350,          # Even higher\n",
    "    'max_depth': 18,            # Deeper trees\n",
    "    'learning_rate': 0.005,     # Very low learning rate\n",
    "    'feature_fraction': 0.98,   # Use almost all features\n",
    "    'bagging_fraction': 0.98,   # Use almost all data\n",
    "    'bagging_freq': 3,\n",
    "    'min_child_samples': 3,     # Very low minimum samples\n",
    "    'min_split_gain': 0.005,    # Very low split gain\n",
    "    'reg_alpha': 0.005,         # Minimal regularization\n",
    "    'reg_lambda': 0.005,\n",
    "    'subsample': 0.98,\n",
    "    'colsample_bytree': 0.98,\n",
    "    'max_bin': 600,             # Even more bins\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'extra_trees': True,\n",
    "    'force_row_wise': True,\n",
    "    'device': 'cpu',\n",
    "    'num_threads': -1,\n",
    "    'min_child_weight': 0.001,\n",
    "    'min_data_in_leaf': 3,\n",
    "    'lambda_l1': 0.005,\n",
    "    'lambda_l2': 0.005\n",
    "}\n",
    "\n",
    "model_lgb_ultra_fine = lgb.train(\n",
    "    params_lgb_ultra_fine,\n",
    "    train_data_lgb,\n",
    "    valid_sets=[valid_data_lgb],\n",
    "    num_boost_round=12000,      # Very high iterations\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(500), # Very high patience\n",
    "        lgb.log_evaluation(0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred_ultra_fine = model_lgb_ultra_fine.predict(X_test_adv, num_iteration=model_lgb_ultra_fine.best_iteration)\n",
    "r2_ultra_fine = r2_score(y_test_adv, y_pred_ultra_fine)\n",
    "mae_ultra_fine = mean_absolute_error(y_test_adv, y_pred_ultra_fine)\n",
    "\n",
    "best_models['Ultra_Fine_LightGBM'] = {\n",
    "    'model': model_lgb_ultra_fine,\n",
    "    'r2': r2_ultra_fine,\n",
    "    'mae': mae_ultra_fine,\n",
    "    'predictions': y_pred_ultra_fine\n",
    "}\n",
    "\n",
    "print(f\"Ultra-Fine LightGBM - RÂ²: {r2_ultra_fine:.4f}, MAE: {mae_ultra_fine:.2f}\")\n",
    "\n",
    "print(\"\\\\nAll optimized models completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c44d0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final results with achieved models...\n",
      "\\n============================================================\n",
      "FINAL MODEL PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Ultra_LightGBM       - RÂ²: 0.6780, MAE: 921.25\n",
      "\\nBest Single Model: Ultra_LightGBM (RÂ² = 0.6780)\n",
      "\\n============================================================\n",
      "ðŸŽ¯ FINAL RESULTS\n",
      "============================================================\n",
      "Best Approach: Ultra_LightGBM\n",
      "Final RÂ² Score: 0.6780\n",
      "Target (â‰¥ 0.68): âŒ CLOSE! Need +0.0020\n",
      "Improvement from baseline: +0.0245\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick ensemble and final results with current best models\n",
    "print(\"Creating final results with achieved models...\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show current best results\n",
    "for model_name, info in best_models.items():\n",
    "    print(f\"{model_name:20} - RÂ²: {info['r2']:.4f}, MAE: {info['mae']:.2f}\")\n",
    "\n",
    "# Find the single best model\n",
    "best_single_model = max(best_models.items(), key=lambda x: x[1]['r2'])\n",
    "best_model_name = best_single_model[0]\n",
    "best_r2_single = best_single_model[1]['r2']\n",
    "\n",
    "print(f\"\\\\nBest Single Model: {best_model_name} (RÂ² = {best_r2_single:.4f})\")\n",
    "\n",
    "# Since we have multiple good LightGBM models, let's ensemble them\n",
    "lightgbm_models = {k: v for k, v in best_models.items() if 'LightGBM' in k}\n",
    "\n",
    "if len(lightgbm_models) > 1:\n",
    "    print(\"\\\\nCreating LightGBM ensemble...\")\n",
    "    \n",
    "    # Weight by RÂ² performance\n",
    "    lgb_r2_scores = [info['r2'] for info in lightgbm_models.values()]\n",
    "    lgb_weights = np.array(lgb_r2_scores) / np.sum(lgb_r2_scores)\n",
    "    \n",
    "    print(\"LightGBM ensemble weights:\")\n",
    "    for i, (name, _) in enumerate(lightgbm_models.items()):\n",
    "        print(f\"  {name}: {lgb_weights[i]:.3f}\")\n",
    "    \n",
    "    # Create ensemble predictions\n",
    "    lgb_predictions_list = [info['predictions'] for info in lightgbm_models.values()]\n",
    "    y_pred_lgb_ensemble = np.average(lgb_predictions_list, axis=0, weights=lgb_weights)\n",
    "    \n",
    "    r2_lgb_ensemble = r2_score(y_test_adv, y_pred_lgb_ensemble)\n",
    "    mae_lgb_ensemble = mean_absolute_error(y_test_adv, y_pred_lgb_ensemble)\n",
    "    \n",
    "    print(f\"\\\\nLightGBM Ensemble - RÂ²: {r2_lgb_ensemble:.4f}, MAE: {mae_lgb_ensemble:.2f}\")\n",
    "    \n",
    "    # Check if ensemble improves performance\n",
    "    if r2_lgb_ensemble > best_r2_single:\n",
    "        final_r2 = r2_lgb_ensemble\n",
    "        final_approach = \"LightGBM_Ensemble\"\n",
    "        use_ensemble = True\n",
    "        print(f\"âœ… Ensemble improves performance!\")\n",
    "    else:\n",
    "        final_r2 = best_r2_single\n",
    "        final_approach = best_model_name\n",
    "        use_ensemble = False\n",
    "        print(f\"Best single model performs better.\")\n",
    "else:\n",
    "    final_r2 = best_r2_single\n",
    "    final_approach = best_model_name\n",
    "    use_ensemble = False\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Approach: {final_approach}\")\n",
    "print(f\"Final RÂ² Score: {final_r2:.4f}\")\n",
    "print(f\"Target (â‰¥ 0.68): {'âœ… ACHIEVED!' if final_r2 >= 0.68 else f'âŒ CLOSE! Need +{(0.68 - final_r2):.4f}'}\")\n",
    "print(f\"Improvement from baseline: +{(final_r2 - 0.6535):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d68768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nGenerating final submission...\n",
      "Test submission shape: (30000, 30)\n",
      "Using best single model: Ultra_LightGBM\n",
      "\\nðŸ“Š SUBMISSION DETAILS\n",
      "File saved as: optimized_lgb_submission.csv\n",
      "Model used: Ultra_LightGBM\n",
      "RÂ² Score achieved: 0.6780\n",
      "Predictions shape: (30000, 2)\n",
      "\\nðŸ“ˆ PRICE STATISTICS\n",
      "Min price: $775.81\n",
      "Max price: $7900.52\n",
      "Mean price: $3143.83\n",
      "Median price: $2512.74\n",
      "Std deviation: $2008.90\n",
      "\\nðŸ” SAMPLE PREDICTIONS\n",
      "      id        price\n",
      "0  20000  1591.245606\n",
      "1  20001  2841.417280\n",
      "2  20002   989.742944\n",
      "3  20003   923.882815\n",
      "4  20004  1274.751776\n",
      "5  20005  1130.139266\n",
      "6  20006  2337.987224\n",
      "7  20007  4544.325464\n",
      "8  20008  4571.125922\n",
      "9  20009  4488.462465\n",
      "\\nðŸ’¾ SAVING MODELS\n",
      "  Saved: ultra-lightgbm-r2-0.6780.joblib\n",
      "\\nðŸ† OPTIMIZATION COMPLETE!\n",
      "Achieved RÂ² = 0.6780\n",
      "Target was RÂ² â‰¥ 0.68\n",
      "ðŸŽ¯ Very close! Only 0.0020 away from target.\n"
     ]
    }
   ],
   "source": [
    "# Generate final submission with best approach\n",
    "print(\"\\\\nGenerating final submission...\")\n",
    "\n",
    "# Prepare test data\n",
    "X_test_submission = test_fresh[feature_cols]\n",
    "print(f\"Test submission shape: {X_test_submission.shape}\")\n",
    "\n",
    "# Generate predictions based on best approach\n",
    "if use_ensemble and len(lightgbm_models) > 1:\n",
    "    print(\"Using LightGBM ensemble for final predictions...\")\n",
    "    final_preds = []\n",
    "    for name, info in lightgbm_models.items():\n",
    "        model = info['model']\n",
    "        pred = model.predict(X_test_submission, num_iteration=model.best_iteration)\n",
    "        final_preds.append(pred)\n",
    "    \n",
    "    submission_predictions = np.average(final_preds, axis=0, weights=lgb_weights)\n",
    "    model_used = \"LightGBM_Ensemble\"\n",
    "else:\n",
    "    print(f\"Using best single model: {best_model_name}\")\n",
    "    best_model = best_models[best_model_name]['model']\n",
    "    submission_predictions = best_model.predict(X_test_submission, num_iteration=best_model.best_iteration)\n",
    "    model_used = best_model_name\n",
    "\n",
    "# Create final submission DataFrame\n",
    "final_submission = pd.DataFrame({\n",
    "    'id': test_fresh['id'],\n",
    "    'price': submission_predictions\n",
    "})\n",
    "\n",
    "# Ensure positive prices and apply reasonable bounds\n",
    "final_submission['price'] = final_submission['price'].clip(lower=0)\n",
    "\n",
    "# Save the submission\n",
    "filename = 'optimized_lgb_submission.csv'\n",
    "final_submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"\\\\nðŸ“Š SUBMISSION DETAILS\")\n",
    "print(f\"File saved as: {filename}\")\n",
    "print(f\"Model used: {model_used}\")\n",
    "print(f\"RÂ² Score achieved: {final_r2:.4f}\")\n",
    "print(f\"Predictions shape: {final_submission.shape}\")\n",
    "\n",
    "print(f\"\\\\nðŸ“ˆ PRICE STATISTICS\")\n",
    "print(f\"Min price: ${final_submission['price'].min():.2f}\")\n",
    "print(f\"Max price: ${final_submission['price'].max():.2f}\")\n",
    "print(f\"Mean price: ${final_submission['price'].mean():.2f}\")\n",
    "print(f\"Median price: ${final_submission['price'].median():.2f}\")\n",
    "print(f\"Std deviation: ${final_submission['price'].std():.2f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(f\"\\\\nðŸ” SAMPLE PREDICTIONS\")\n",
    "print(final_submission.head(10))\n",
    "\n",
    "# Save the best model(s)\n",
    "import joblib\n",
    "\n",
    "print(f\"\\\\nðŸ’¾ SAVING MODELS\")\n",
    "if use_ensemble and len(lightgbm_models) > 1:\n",
    "    for i, (name, info) in enumerate(lightgbm_models.items()):\n",
    "        safe_name = name.lower().replace('_', '-').replace(' ', '-')\n",
    "        filename_model = f'{safe_name}-r2-{info[\"r2\"]:.4f}.joblib'\n",
    "        joblib.dump(info['model'], filename_model)\n",
    "        print(f\"  Saved: {filename_model}\")\n",
    "else:\n",
    "    safe_name = best_model_name.lower().replace('_', '-').replace(' ', '-')\n",
    "    filename_model = f'{safe_name}-r2-{final_r2:.4f}.joblib'\n",
    "    joblib.dump(best_models[best_model_name]['model'], filename_model)\n",
    "    print(f\"  Saved: {filename_model}\")\n",
    "\n",
    "print(\"\\\\nðŸ† OPTIMIZATION COMPLETE!\")\n",
    "print(f\"Achieved RÂ² = {final_r2:.4f}\")\n",
    "print(f\"Target was RÂ² â‰¥ 0.68\")\n",
    "if final_r2 >= 0.68:\n",
    "    print(\"ðŸŽ‰ TARGET ACHIEVED!\")\n",
    "else:\n",
    "    print(f\"ðŸŽ¯ Very close! Only {(0.68 - final_r2):.4f} away from target.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "213a5207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ COMPLETE OPTIMIZED DIAMOND PRICE PREDICTION MODEL\n",
      "============================================================\n",
      "ðŸ“Š Loading and preprocessing data...\n",
      "Original train shape: (20000, 11), test shape: (30000, 10)\n",
      "Removed 1758 price outliers\n",
      "Final training data shape: (18239, 11)\n",
      "âœ… Data preprocessing completed!\n",
      "ðŸ”§ Creating advanced features...\n",
      "âœ… Advanced feature engineering completed!\n",
      "ðŸŽ¯ Training ultra-optimized LightGBM model...\n",
      "Feature matrix shape: (18239, 30)\n",
      "Target shape: (18239,)\n",
      "Train set: (14591, 30), Test set: (3648, 30)\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "âœ… Data preprocessing completed!\n",
      "ðŸ”§ Creating advanced features...\n",
      "âœ… Advanced feature engineering completed!\n",
      "ðŸŽ¯ Training ultra-optimized LightGBM model...\n",
      "Feature matrix shape: (18239, 30)\n",
      "Target shape: (18239,)\n",
      "Train set: (14591, 30), Test set: (3648, 30)\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's rmse: 1609.04\n",
      "[100]\tvalid_0's rmse: 1609.04\n",
      "[200]\tvalid_0's rmse: 1443.63\n",
      "[200]\tvalid_0's rmse: 1443.63\n",
      "[300]\tvalid_0's rmse: 1420.21\n",
      "[300]\tvalid_0's rmse: 1420.21\n",
      "[400]\tvalid_0's rmse: 1419.03\n",
      "[400]\tvalid_0's rmse: 1419.03\n",
      "[500]\tvalid_0's rmse: 1420.69\n",
      "[500]\tvalid_0's rmse: 1420.69\n",
      "[600]\tvalid_0's rmse: 1423.91\n",
      "[600]\tvalid_0's rmse: 1423.91\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's rmse: 1418.22\n",
      "ðŸŽ‰ FINAL MODEL PERFORMANCE:\n",
      "   RÂ² Score: 0.6780\n",
      "   MAE: 921.25\n",
      "   Best iteration: 362\n",
      "ðŸ“ Generating final predictions...\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's rmse: 1418.22\n",
      "ðŸŽ‰ FINAL MODEL PERFORMANCE:\n",
      "   RÂ² Score: 0.6780\n",
      "   MAE: 921.25\n",
      "   Best iteration: 362\n",
      "ðŸ“ Generating final predictions...\n",
      "ðŸ’¾ FILES SAVED:\n",
      "   Submission: diamond_prediction_r2_0678.csv\n",
      "   Model: ultra_lightgbm_r2_0678.joblib\n",
      "\\nðŸ“Š PREDICTION STATISTICS:\n",
      "   Min price: $775.81\n",
      "   Max price: $7900.52\n",
      "   Mean price: $3143.83\n",
      "   Median price: $2512.74\n",
      "\\n============================================================\n",
      "ðŸ† COMPLETE MODEL EXECUTION FINISHED!\n",
      "ðŸŽ¯ Achieved RÂ² Score: 0.6780\n",
      "ðŸ“ Output: diamond_prediction_r2_0678.csv\n",
      "============================================================\n",
      "ðŸ’¾ FILES SAVED:\n",
      "   Submission: diamond_prediction_r2_0678.csv\n",
      "   Model: ultra_lightgbm_r2_0678.joblib\n",
      "\\nðŸ“Š PREDICTION STATISTICS:\n",
      "   Min price: $775.81\n",
      "   Max price: $7900.52\n",
      "   Mean price: $3143.83\n",
      "   Median price: $2512.74\n",
      "\\n============================================================\n",
      "ðŸ† COMPLETE MODEL EXECUTION FINISHED!\n",
      "ðŸŽ¯ Achieved RÂ² Score: 0.6780\n",
      "ðŸ“ Output: diamond_prediction_r2_0678.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE OPTIMIZED DIAMOND PRICE PREDICTION MODEL - RÂ² = 0.6780\n",
    "# This cell contains all the code that achieved the 0.6780 RÂ² score in one complete block\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸš€ COMPLETE OPTIMIZED DIAMOND PRICE PREDICTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š Loading and preprocessing data...\")\n",
    "\n",
    "# Load fresh data\n",
    "train_data = pd.read_csv('/home/ayushz/Kaggle/Datasets/train.csv')\n",
    "test_data = pd.read_csv('/home/ayushz/Kaggle/Datasets/test.csv')\n",
    "\n",
    "print(f\"Original train shape: {train_data.shape}, test shape: {test_data.shape}\")\n",
    "\n",
    "# Advanced outlier removal using IQR method\n",
    "def remove_outliers_iqr(df, column, factor=1.5):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Remove price outliers\n",
    "original_len = len(train_data)\n",
    "train_data = remove_outliers_iqr(train_data, 'price', factor=1.2)\n",
    "print(f\"Removed {original_len - len(train_data)} price outliers\")\n",
    "\n",
    "# Remove dimension outliers\n",
    "for dim in ['x', 'y', 'z']:\n",
    "    train_data = remove_outliers_iqr(train_data, dim, factor=2.0)\n",
    "\n",
    "print(f\"Final training data shape: {train_data.shape}\")\n",
    "\n",
    "# Advanced missing value handling\n",
    "for col in ['x','y','z']:\n",
    "    # Replace very small and zero values with NaN\n",
    "    train_data.loc[train_data[col] <= 0.01, col] = np.nan\n",
    "    test_data.loc[test_data[col] <= 0.01, col] = np.nan\n",
    "    \n",
    "    # Group-based imputation by cut and clarity\n",
    "    for cut_val in train_data['cut'].unique():\n",
    "        for clarity_val in train_data['clarity'].unique():\n",
    "            mask_train = (train_data['cut'] == cut_val) & (train_data['clarity'] == clarity_val)\n",
    "            mask_test = (test_data['cut'] == cut_val) & (test_data['clarity'] == clarity_val)\n",
    "            \n",
    "            if mask_train.sum() > 0 and train_data.loc[mask_train, col].notna().sum() > 0:\n",
    "                median_val = train_data.loc[mask_train, col].median()\n",
    "                train_data.loc[mask_train & train_data[col].isna(), col] = median_val\n",
    "                test_data.loc[mask_test & test_data[col].isna(), col] = median_val\n",
    "    \n",
    "    # Fill remaining with overall median\n",
    "    overall_median = train_data[col].median()\n",
    "    train_data[col].fillna(overall_median, inplace=True)\n",
    "    test_data[col].fillna(overall_median, inplace=True)\n",
    "\n",
    "# Clean categorical variables\n",
    "train_data['cut'] = train_data['cut'].str.strip()\n",
    "test_data['cut'] = test_data['cut'].str.strip()\n",
    "\n",
    "print(\"âœ… Data preprocessing completed!\")\n",
    "\n",
    "\n",
    "print(\"ðŸ”§ Creating advanced features...\")\n",
    "\n",
    "# Consistent label encoding\n",
    "le_cut = LabelEncoder()\n",
    "le_color = LabelEncoder()\n",
    "le_clarity = LabelEncoder()\n",
    "\n",
    "# Fit on combined data for consistency\n",
    "all_data = pd.concat([\n",
    "    train_data[['cut', 'color', 'clarity']], \n",
    "    test_data[['cut', 'color', 'clarity']]\n",
    "])\n",
    "\n",
    "le_cut.fit(all_data['cut'])\n",
    "le_color.fit(all_data['color'])\n",
    "le_clarity.fit(all_data['clarity'])\n",
    "\n",
    "train_data['cut_encoded'] = le_cut.transform(train_data['cut'])\n",
    "train_data['color_encoded'] = le_color.transform(train_data['color'])\n",
    "train_data['clarity_encoded'] = le_clarity.transform(train_data['clarity'])\n",
    "\n",
    "test_data['cut_encoded'] = le_cut.transform(test_data['cut'])\n",
    "test_data['color_encoded'] = le_color.transform(test_data['color'])\n",
    "test_data['clarity_encoded'] = le_clarity.transform(test_data['clarity'])\n",
    "\n",
    "# Basic geometric features\n",
    "train_data['volume'] = train_data['x'] * train_data['y'] * train_data['z']\n",
    "test_data['volume'] = test_data['x'] * test_data['y'] * test_data['z']\n",
    "\n",
    "train_data['surface_area'] = 2 * (train_data['x']*train_data['y'] + \n",
    "                                 train_data['x']*train_data['z'] + \n",
    "                                 train_data['y']*train_data['z'])\n",
    "test_data['surface_area'] = 2 * (test_data['x']*test_data['y'] + \n",
    "                                test_data['x']*test_data['z'] + \n",
    "                                test_data['y']*test_data['z'])\n",
    "\n",
    "# Advanced ratios and transformations\n",
    "train_data['carat_per_volume'] = train_data['carat'] / (train_data['volume'] + 1e-8)\n",
    "test_data['carat_per_volume'] = test_data['carat'] / (test_data['volume'] + 1e-8)\n",
    "\n",
    "train_data['density'] = train_data['carat'] / (train_data['volume'] + 1e-8)\n",
    "test_data['density'] = test_data['carat'] / (test_data['volume'] + 1e-8)\n",
    "\n",
    "train_data['aspect_xy'] = train_data['x'] / (train_data['y'] + 1e-8)\n",
    "test_data['aspect_xy'] = test_data['x'] / (test_data['y'] + 1e-8)\n",
    "\n",
    "train_data['aspect_xz'] = train_data['x'] / (train_data['z'] + 1e-8)\n",
    "test_data['aspect_xz'] = test_data['x'] / (test_data['z'] + 1e-8)\n",
    "\n",
    "# Domain knowledge quality scores\n",
    "cut_quality = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n",
    "color_quality = {'J': 1, 'I': 2, 'H': 3, 'G': 4, 'F': 5, 'E': 6, 'D': 7}\n",
    "clarity_quality = {'I1': 1, 'SI2': 2, 'SI1': 3, 'VS2': 4, 'VS1': 5, 'VVS2': 6, 'VVS1': 7, 'IF': 8}\n",
    "\n",
    "train_data['cut_quality'] = train_data['cut'].map(cut_quality).fillna(3)\n",
    "test_data['cut_quality'] = test_data['cut'].map(cut_quality).fillna(3)\n",
    "\n",
    "train_data['color_quality'] = train_data['color'].map(color_quality).fillna(4)\n",
    "test_data['color_quality'] = test_data['color'].map(color_quality).fillna(4)\n",
    "\n",
    "train_data['clarity_quality'] = train_data['clarity'].map(clarity_quality).fillna(4)\n",
    "test_data['clarity_quality'] = test_data['clarity'].map(clarity_quality).fillna(4)\n",
    "\n",
    "train_data['overall_quality'] = (train_data['cut_quality'] + \n",
    "                                train_data['color_quality'] + \n",
    "                                train_data['clarity_quality'])\n",
    "test_data['overall_quality'] = (test_data['cut_quality'] + \n",
    "                               test_data['color_quality'] + \n",
    "                               test_data['clarity_quality'])\n",
    "\n",
    "# Log transformations\n",
    "train_data['log_carat'] = np.log1p(train_data['carat'])\n",
    "test_data['log_carat'] = np.log1p(test_data['carat'])\n",
    "\n",
    "train_data['log_volume'] = np.log1p(train_data['volume'])\n",
    "test_data['log_volume'] = np.log1p(test_data['volume'])\n",
    "\n",
    "# Polynomial features\n",
    "for col in ['carat', 'volume', 'overall_quality']:\n",
    "    train_data[f'{col}_squared'] = train_data[col] ** 2\n",
    "    test_data[f'{col}_squared'] = test_data[col] ** 2\n",
    "    \n",
    "    train_data[f'{col}_cubed'] = train_data[col] ** 3\n",
    "    test_data[f'{col}_cubed'] = test_data[col] ** 3\n",
    "\n",
    "# Interaction features\n",
    "train_data['carat_quality'] = train_data['carat'] * train_data['overall_quality']\n",
    "test_data['carat_quality'] = test_data['carat'] * test_data['overall_quality']\n",
    "\n",
    "train_data['volume_quality'] = train_data['volume'] * train_data['overall_quality']\n",
    "test_data['volume_quality'] = test_data['volume'] * test_data['overall_quality']\n",
    "\n",
    "# Binning features\n",
    "train_data['carat_bin'] = pd.cut(train_data['carat'], bins=10, labels=False)\n",
    "test_data['carat_bin'] = pd.cut(test_data['carat'], bins=10, labels=False)\n",
    "\n",
    "print(\"âœ… Advanced feature engineering completed!\")\n",
    "\n",
    "\n",
    "print(\"ðŸŽ¯ Training ultra-optimized LightGBM model...\")\n",
    "\n",
    "# Feature selection\n",
    "feature_cols = [\n",
    "    'carat', 'cut_encoded', 'color_encoded', 'clarity_encoded', 'depth', 'table',\n",
    "    'x', 'y', 'z', 'volume', 'surface_area', 'carat_per_volume', 'density',\n",
    "    'aspect_xy', 'aspect_xz', 'cut_quality', 'color_quality', 'clarity_quality',\n",
    "    'overall_quality', 'log_carat', 'log_volume', 'carat_squared', 'volume_squared',\n",
    "    'overall_quality_squared', 'carat_cubed', 'volume_cubed', 'overall_quality_cubed',\n",
    "    'carat_quality', 'volume_quality', 'carat_bin'\n",
    "]\n",
    "\n",
    "X = train_data[feature_cols]\n",
    "y = train_data['price']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, \n",
    "    stratify=pd.cut(y, bins=10, labels=False)\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Ultra-optimized LightGBM parameters (achieved RÂ² = 0.6780)\n",
    "params_ultra_lgb = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 300,          # High complexity\n",
    "    'max_depth': 15,            # Deep trees\n",
    "    'learning_rate': 0.01,      # Low learning rate for stability\n",
    "    'feature_fraction': 0.95,   # Use most features\n",
    "    'bagging_fraction': 0.95,   # Use most data\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 5,     # Allow small leaves\n",
    "    'min_split_gain': 0.01,     # Easy splitting\n",
    "    'reg_alpha': 0.01,          # Minimal regularization\n",
    "    'reg_lambda': 0.01,\n",
    "    'subsample': 0.95,\n",
    "    'colsample_bytree': 0.95,\n",
    "    'max_bin': 500,             # High granularity\n",
    "    'verbose': -1,\n",
    "    'random_state': 42,\n",
    "    'extra_trees': True,        # Extra randomization\n",
    "    'force_row_wise': True,\n",
    "    'device': 'cpu',\n",
    "    'num_threads': -1\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train)\n",
    "valid_lgb = lgb.Dataset(X_test, label=y_test, reference=train_lgb)\n",
    "\n",
    "# Train the model\n",
    "model_final = lgb.train(\n",
    "    params_ultra_lgb,\n",
    "    train_lgb,\n",
    "    valid_sets=[valid_lgb],\n",
    "    num_boost_round=8000,       # High iterations\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(300), # High patience\n",
    "        lgb.log_evaluation(100)   # Show progress\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model_final.predict(X_test, num_iteration=model_final.best_iteration)\n",
    "r2_final = r2_score(y_test, y_pred)\n",
    "mae_final = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"ðŸŽ‰ FINAL MODEL PERFORMANCE:\")\n",
    "print(f\"   RÂ² Score: {r2_final:.4f}\")\n",
    "print(f\"   MAE: {mae_final:.2f}\")\n",
    "print(f\"   Best iteration: {model_final.best_iteration}\")\n",
    "\n",
    "\n",
    "print(\"ðŸ“ Generating final predictions...\")\n",
    "\n",
    "# Prepare test data\n",
    "X_test_final = test_data[feature_cols]\n",
    "\n",
    "# Generate predictions\n",
    "final_predictions = model_final.predict(X_test_final, num_iteration=model_final.best_iteration)\n",
    "\n",
    "# Create submission\n",
    "submission_final = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure positive prices\n",
    "submission_final['price'] = submission_final['price'].clip(lower=0)\n",
    "\n",
    "# Save files\n",
    "submission_file = 'diamond_prediction_r2_0678.csv'\n",
    "model_file = 'ultra_lightgbm_r2_0678.joblib'\n",
    "\n",
    "submission_final.to_csv(submission_file, index=False)\n",
    "joblib.dump(model_final, model_file)\n",
    "\n",
    "print(f\"ðŸ’¾ FILES SAVED:\")\n",
    "print(f\"   Submission: {submission_file}\")\n",
    "print(f\"   Model: {model_file}\")\n",
    "\n",
    "print(f\"\\\\nðŸ“Š PREDICTION STATISTICS:\")\n",
    "print(f\"   Min price: ${submission_final['price'].min():.2f}\")\n",
    "print(f\"   Max price: ${submission_final['price'].max():.2f}\")\n",
    "print(f\"   Mean price: ${submission_final['price'].mean():.2f}\")\n",
    "print(f\"   Median price: ${submission_final['price'].median():.2f}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"ðŸ† COMPLETE MODEL EXECUTION FINISHED!\")\n",
    "print(f\"ðŸŽ¯ Achieved RÂ² Score: {r2_final:.4f}\")\n",
    "print(f\"ðŸ“ Output: {submission_file}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c9ef21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train shape: (20000, 11)\n",
      "Original test shape: (30000, 10)\n",
      "Removed 450 outliers from training data\n",
      "Data preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Improved Diamond Price Prediction Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/home/ayushz/Kaggle/Datasets/train.csv')\n",
    "test = pd.read_csv('/home/ayushz/Kaggle/Datasets/test.csv')\n",
    "\n",
    "print(\"Original train shape:\", train.shape)\n",
    "print(\"Original test shape:\", test.shape)\n",
    "\n",
    "# Better outlier detection and handling\n",
    "def remove_outliers(df, columns, z_threshold=3):\n",
    "    \"\"\"Remove outliers using Z-score method\"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "            df = df[z_scores < z_threshold]\n",
    "    return df\n",
    "\n",
    "# Remove price outliers from training data\n",
    "train_clean = remove_outliers(train, ['price'], z_threshold=3)\n",
    "print(f\"Removed {len(train) - len(train_clean)} outliers from training data\")\n",
    "train = train_clean.copy()\n",
    "\n",
    "# Improved missing value handling for dimensions\n",
    "for col in ['x','y','z']:\n",
    "    # Replace 0s and very small values with NaN\n",
    "    train.loc[train[col] <= 0.01, col] = np.nan\n",
    "    test.loc[test[col] <= 0.01, col] = np.nan\n",
    "    \n",
    "    # Fill with median grouped by cut and carat quartiles\n",
    "    train['carat_quartile'] = pd.qcut(train['carat'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "    test['carat_quartile'] = pd.qcut(test['carat'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "    \n",
    "    for cut in train['cut'].unique():\n",
    "        for quartile in train['carat_quartile'].unique():\n",
    "            mask_train = (train['cut'] == cut) & (train['carat_quartile'] == quartile)\n",
    "            mask_test = (test['cut'] == cut) & (test['carat_quartile'] == quartile)\n",
    "            \n",
    "            if mask_train.sum() > 0:\n",
    "                median_val = train.loc[mask_train, col].median()\n",
    "                if pd.isna(median_val):\n",
    "                    median_val = train[col].median()\n",
    "                train.loc[mask_train & train[col].isna(), col] = median_val\n",
    "                test.loc[mask_test & test[col].isna(), col] = median_val\n",
    "\n",
    "# Fill remaining NaN values with overall median\n",
    "for col in ['x','y','z']:\n",
    "    train[col].fillna(train[col].median(), inplace=True)\n",
    "    test[col].fillna(train[col].median(), inplace=True)\n",
    "\n",
    "# Clean categorical variables\n",
    "train['cut'] = train['cut'].str.strip()\n",
    "test['cut'] = test['cut'].str.strip()\n",
    "\n",
    "# Consistent label encoding\n",
    "le_cut = LabelEncoder()\n",
    "le_color = LabelEncoder()\n",
    "le_clarity = LabelEncoder()\n",
    "\n",
    "# Fit on combined data to ensure consistency\n",
    "all_cuts = list(train['cut'].unique()) + list(test['cut'].unique())\n",
    "all_colors = list(train['color'].unique()) + list(test['color'].unique())\n",
    "all_clarity = list(train['clarity'].unique()) + list(test['clarity'].unique())\n",
    "\n",
    "le_cut.fit(list(set(all_cuts)))\n",
    "le_color.fit(list(set(all_colors)))\n",
    "le_clarity.fit(list(set(all_clarity)))\n",
    "\n",
    "train['cut_encoded'] = le_cut.transform(train['cut'])\n",
    "train['color_encoded'] = le_color.transform(train['color'])\n",
    "train['clarity_encoded'] = le_clarity.transform(train['clarity'])\n",
    "\n",
    "test['cut_encoded'] = le_cut.transform(test['cut'])\n",
    "test['color_encoded'] = le_color.transform(test['color'])\n",
    "test['clarity_encoded'] = le_clarity.transform(test['clarity'])\n",
    "\n",
    "print(\"Data preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ee7c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced feature engineering completed!\n",
      "Training features shape: (19550, 28)\n",
      "Test features shape: (30000, 26)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Feature Engineering\n",
    "# Basic volume and ratios\n",
    "train['volume'] = train['x'] * train['y'] * train['z']\n",
    "test['volume'] = test['x'] * test['y'] * test['z']\n",
    "\n",
    "# Carat per volume ratio\n",
    "train['carat_per_volume'] = train['carat'] / (train['volume'] + 1e-9)\n",
    "test['carat_per_volume'] = test['carat'] / (test['volume'] + 1e-9)\n",
    "\n",
    "# Additional geometric features\n",
    "train['surface_area'] = 2 * (train['x']*train['y'] + train['x']*train['z'] + train['y']*train['z'])\n",
    "test['surface_area'] = 2 * (test['x']*test['y'] + test['x']*test['z'] + test['y']*test['z'])\n",
    "\n",
    "train['aspect_ratio_xy'] = train['x'] / (train['y'] + 1e-9)\n",
    "test['aspect_ratio_xy'] = test['x'] / (test['y'] + 1e-9)\n",
    "\n",
    "train['aspect_ratio_xz'] = train['x'] / (train['z'] + 1e-9)\n",
    "test['aspect_ratio_xz'] = test['x'] / (test['z'] + 1e-9)\n",
    "\n",
    "# Depth and table ratios (important for diamond quality)\n",
    "train['depth_ratio'] = train['depth'] / 100.0\n",
    "test['depth_ratio'] = test['depth'] / 100.0\n",
    "\n",
    "train['table_ratio'] = train['table'] / 100.0\n",
    "test['table_ratio'] = test['table'] / 100.0\n",
    "\n",
    "# Carat interactions with categorical features\n",
    "train['carat_cut_interaction'] = train['carat'] * train['cut_encoded']\n",
    "test['carat_cut_interaction'] = test['carat'] * test['cut_encoded']\n",
    "\n",
    "train['carat_color_interaction'] = train['carat'] * train['color_encoded']\n",
    "test['carat_color_interaction'] = test['carat'] * test['color_encoded']\n",
    "\n",
    "train['carat_clarity_interaction'] = train['carat'] * train['clarity_encoded']\n",
    "test['carat_clarity_interaction'] = test['carat'] * test['clarity_encoded']\n",
    "\n",
    "# Quality score based on cut, color, clarity\n",
    "train['quality_score'] = train['cut_encoded'] + train['color_encoded'] + train['clarity_encoded']\n",
    "test['quality_score'] = test['cut_encoded'] + test['color_encoded'] + test['clarity_encoded']\n",
    "\n",
    "# Volume to carat ratio (density indicator)\n",
    "train['density'] = train['carat'] / (train['volume'] + 1e-9)\n",
    "test['density'] = test['carat'] / (test['volume'] + 1e-9)\n",
    "\n",
    "# Price per carat for training (useful for understanding patterns)\n",
    "train['price_per_carat'] = train['price'] / train['carat']\n",
    "\n",
    "print(\"Enhanced feature engineering completed!\")\n",
    "print(f\"Training features shape: {train.shape}\")\n",
    "print(f\"Test features shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c327d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (15640, 21)\n",
      "Test set shape: (3910, 21)\n",
      "Training optimized LightGBM model...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's rmse: 2014.04\n",
      "Optimized LightGBM MAE: 1231.25\n",
      "Optimized LightGBM RÂ² Score: 0.6505\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                      feature    importance\n",
      "0                       carat  6.509195e+11\n",
      "9                      volume  1.745835e+11\n",
      "6                           x  1.465852e+11\n",
      "11               surface_area  3.833265e+10\n",
      "8                           z  3.600991e+10\n",
      "7                           y  3.493062e+10\n",
      "13            aspect_ratio_xz  1.204206e+10\n",
      "10           carat_per_volume  1.177935e+10\n",
      "18  carat_clarity_interaction  1.168009e+10\n",
      "4                       depth  1.105688e+10\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for modeling (excluding unnecessary columns)\n",
    "feature_columns = [\n",
    "    'carat', 'cut_encoded', 'color_encoded', 'clarity_encoded', 'depth', 'table',\n",
    "    'x', 'y', 'z', 'volume', 'carat_per_volume', 'surface_area', \n",
    "    'aspect_ratio_xy', 'aspect_ratio_xz', 'depth_ratio', 'table_ratio',\n",
    "    'carat_cut_interaction', 'carat_color_interaction', 'carat_clarity_interaction',\n",
    "    'quality_score', 'density'\n",
    "]\n",
    "\n",
    "X = train[feature_columns]\n",
    "y = train['price']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "# Create LightGBM datasets\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Optimized hyperparameters for better performance\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 100,           # Increased from 65\n",
    "    'max_depth': 7,              # Increased from 3\n",
    "    'learning_rate': 0.05,       # Decreased for better generalization\n",
    "    'subsample': 0.8,            # Slightly decreased\n",
    "    'colsample_bytree': 0.8,     # Increased from 0.7\n",
    "    'min_child_samples': 20,     # Decreased from 55\n",
    "    'min_split_gain': 0.1,       # Decreased from 0.2222\n",
    "    'reg_alpha': 0.1,            # Decreased from 0.2222\n",
    "    'reg_lambda': 0.1,           # Increased from 0.0\n",
    "    'max_bin': 255,              # Increased from 250\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'force_row_wise': True,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Training optimized LightGBM model...\")\n",
    "\n",
    "# Train the model with early stopping\n",
    "model_lgb_optimized = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[test_data],\n",
    "    num_boost_round=2000,        # Increased from 1000\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(100),  # Increased patience\n",
    "        lgb.log_evaluation(200)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_optimized = model_lgb_optimized.predict(X_test, num_iteration=model_lgb_optimized.best_iteration)\n",
    "mae_optimized = mean_absolute_error(y_test, y_pred_optimized)\n",
    "r2_optimized = r2_score(y_test, y_pred_optimized)\n",
    "\n",
    "print(f\"Optimized LightGBM MAE: {mae_optimized:.2f}\")\n",
    "print(f\"Optimized LightGBM RÂ² Score: {r2_optimized:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = model_lgb_optimized.feature_importance(importance_type='gain')\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4330d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate improved submission\n",
    "print(\"Generating improved submission...\")\n",
    "\n",
    "# Prepare test data for submission\n",
    "X_submission = test[feature_columns]\n",
    "\n",
    "# Predict on the competition test set\n",
    "competition_predictions_optimized = model_lgb_optimized.predict(\n",
    "    X_submission, \n",
    "    num_iteration=model_lgb_optimized.best_iteration\n",
    ")\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df_optimized = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'price': competition_predictions_optimized\n",
    "})\n",
    "\n",
    "# Ensure prices are not negative\n",
    "submission_df_optimized['price'] = submission_df_optimized['price'].clip(0)\n",
    "\n",
    "# Save the improved submission file\n",
    "submission_df_optimized.to_csv('improved_submission.csv', index=False)\n",
    "\n",
    "print(\"Improved submission file 'improved_submission.csv' created successfully!\")\n",
    "print(f\"RÂ² Score achieved: {r2_optimized:.4f}\")\n",
    "print(f\"Target RÂ² Score: 0.70\")\n",
    "print(f\"Goal achieved: {'YES' if r2_optimized >= 0.70 else 'NO'}\")\n",
    "\n",
    "print(\"\\nSubmission file preview:\")\n",
    "print(submission_df_optimized.head(10))\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"Min price: ${submission_df_optimized['price'].min():.2f}\")\n",
    "print(f\"Max price: ${submission_df_optimized['price'].max():.2f}\")\n",
    "print(f\"Mean price: ${submission_df_optimized['price'].mean():.2f}\")\n",
    "print(f\"Median price: ${submission_df_optimized['price'].median():.2f}\")\n",
    "\n",
    "# Save the model\n",
    "import joblib\n",
    "joblib.dump(model_lgb_optimized, 'improved_lgbm_model.joblib')\n",
    "print(\"Improved model saved as 'improved_lgbm_model.joblib'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1090b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble models for improved performance...\n",
      "XGBoost RÂ² Score: 0.6163\n",
      "XGBoost MAE: 1300.52\n",
      "Random Forest RÂ² Score: 0.6424\n",
      "Random Forest MAE: 1243.63\n",
      "Model weights: LGB=0.341, XGB=0.323, RF=0.336\n",
      "Ensemble RÂ² Score: 0.6440\n",
      "Ensemble MAE: 1242.68\n",
      "\n",
      "Best model: LightGBM with RÂ² = 0.6505\n",
      "Target achieved (RÂ² â‰¥ 0.70): NO\n"
     ]
    }
   ],
   "source": [
    "# Let's try XGBoost and Random Forest ensemble for better performance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Training ensemble models for improved performance...\")\n",
    "\n",
    "# XGBoost model\n",
    "xgb_params = {\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 500,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost RÂ² Score: {r2_xgb:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.2f}\")\n",
    "\n",
    "# Random Forest model\n",
    "rf_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 15,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "model_rf = RandomForestRegressor(**rf_params)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest RÂ² Score: {r2_rf:.4f}\")\n",
    "print(f\"Random Forest MAE: {mae_rf:.2f}\")\n",
    "\n",
    "# Ensemble prediction (weighted average)\n",
    "# Weight models based on their RÂ² performance\n",
    "weights = [r2_optimized, r2_xgb, r2_rf]\n",
    "weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "print(f\"Model weights: LGB={weights[0]:.3f}, XGB={weights[1]:.3f}, RF={weights[2]:.3f}\")\n",
    "\n",
    "y_pred_ensemble = (weights[0] * y_pred_optimized + \n",
    "                  weights[1] * y_pred_xgb + \n",
    "                  weights[2] * y_pred_rf)\n",
    "\n",
    "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
    "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "\n",
    "print(f\"Ensemble RÂ² Score: {r2_ensemble:.4f}\")\n",
    "print(f\"Ensemble MAE: {mae_ensemble:.2f}\")\n",
    "\n",
    "# Choose the best model\n",
    "models = {\n",
    "    'LightGBM': (model_lgb_optimized, r2_optimized, y_pred_optimized),\n",
    "    'XGBoost': (model_xgb, r2_xgb, y_pred_xgb),\n",
    "    'Random Forest': (model_rf, r2_rf, y_pred_rf),\n",
    "    'Ensemble': (None, r2_ensemble, y_pred_ensemble)\n",
    "}\n",
    "\n",
    "best_model_name = max(models.keys(), key=lambda x: models[x][1])\n",
    "best_r2 = models[best_model_name][1]\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} with RÂ² = {best_r2:.4f}\")\n",
    "print(f\"Target achieved (RÂ² â‰¥ 0.70): {'YES' if best_r2 >= 0.70 else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final submission with best performing model\n",
    "print(\"Generating final optimized submission...\")\n",
    "\n",
    "# Get predictions from all models on test set\n",
    "X_submission = test[feature_columns]\n",
    "\n",
    "lgb_submission_pred = model_lgb_optimized.predict(X_submission, num_iteration=model_lgb_optimized.best_iteration)\n",
    "xgb_submission_pred = model_xgb.predict(X_submission)\n",
    "rf_submission_pred = model_rf.predict(X_submission)\n",
    "\n",
    "# Ensemble prediction for submission\n",
    "final_predictions = (weights[0] * lgb_submission_pred + \n",
    "                    weights[1] * xgb_submission_pred + \n",
    "                    weights[2] * rf_submission_pred)\n",
    "\n",
    "# Create final submission\n",
    "final_submission_df = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure prices are not negative\n",
    "final_submission_df['price'] = final_submission_df['price'].clip(0)\n",
    "\n",
    "# Save the final submission\n",
    "final_submission_df.to_csv('final_optimized_submission.csv', index=False)\n",
    "\n",
    "print(\"Final optimized submission saved as 'final_optimized_submission.csv'\")\n",
    "print(f\"Best RÂ² Score achieved: {best_r2:.4f}\")\n",
    "print(f\"Target RÂ² (â‰¥ 0.70): {'âœ“ ACHIEVED' if best_r2 >= 0.70 else 'âœ— NOT ACHIEVED'}\")\n",
    "\n",
    "print(\"\\nFinal submission preview:\")\n",
    "print(final_submission_df.head())\n",
    "\n",
    "print(f\"\\nFinal submission statistics:\")\n",
    "print(f\"Min price: ${final_submission_df['price'].min():.2f}\")\n",
    "print(f\"Max price: ${final_submission_df['price'].max():.2f}\")\n",
    "print(f\"Mean price: ${final_submission_df['price'].mean():.2f}\")\n",
    "print(f\"Median price: ${final_submission_df['price'].median():.2f}\")\n",
    "\n",
    "# Save all models\n",
    "import joblib\n",
    "joblib.dump(model_xgb, 'xgb_model_optimized.joblib')\n",
    "joblib.dump(model_rf, 'rf_model_optimized.joblib')\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")\n",
    "print(\"- improved_lgbm_model.joblib\")\n",
    "print(\"- xgb_model_optimized.joblib\") \n",
    "print(\"- rf_model_optimized.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3e8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting final aggressive optimization...\n",
      "Enhanced feature set size: 33 features\n",
      "Training ultra-optimized model...\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's rmse: 2006.9\n",
      "Ultra-optimized RÂ² Score: 0.6529\n",
      "Ultra-optimized MAE: 1222.96\n",
      "Target achieved (RÂ² â‰¥ 0.70): NO âœ—\n",
      "New best model: Ultra-optimized LightGBM with RÂ² = 0.6529\n"
     ]
    }
   ],
   "source": [
    "# Final aggressive optimization attempt\n",
    "print(\"Attempting final aggressive optimization...\")\n",
    "\n",
    "# Try polynomial features for key variables\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features for the most important features\n",
    "poly_features = ['carat', 'x', 'y', 'z', 'volume']\n",
    "X_train_poly = X_train.copy()\n",
    "X_test_poly = X_test.copy()\n",
    "\n",
    "for feature in poly_features:\n",
    "    X_train_poly[f'{feature}_squared'] = X_train[feature] ** 2\n",
    "    X_test_poly[f'{feature}_squared'] = X_test[feature] ** 2\n",
    "    X_train_poly[f'{feature}_cubed'] = X_train[feature] ** 3\n",
    "    X_test_poly[f'{feature}_cubed'] = X_test[feature] ** 3\n",
    "\n",
    "# Add more interaction features\n",
    "X_train_poly['carat_volume_interaction'] = X_train['carat'] * X_train['volume']\n",
    "X_test_poly['carat_volume_interaction'] = X_test['carat'] * X_test['volume']\n",
    "\n",
    "X_train_poly['xyz_product'] = X_train['x'] * X_train['y'] * X_train['z']\n",
    "X_test_poly['xyz_product'] = X_test['x'] * X_test['y'] * X_test['z']\n",
    "\n",
    "print(f\"Enhanced feature set size: {X_train_poly.shape[1]} features\")\n",
    "\n",
    "# Create new LightGBM datasets with polynomial features\n",
    "train_data_poly = lgb.Dataset(X_train_poly, label=y_train)\n",
    "test_data_poly = lgb.Dataset(X_test_poly, label=y_test, reference=train_data_poly)\n",
    "\n",
    "# Ultra-optimized parameters\n",
    "params_ultra = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 150,          # Increased further\n",
    "    'max_depth': 10,            # Increased\n",
    "    'learning_rate': 0.03,      # Decreased for more iterations\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'min_child_samples': 10,    # Decreased\n",
    "    'min_split_gain': 0.05,     # Decreased\n",
    "    'reg_alpha': 0.05,          # Decreased\n",
    "    'reg_lambda': 0.05,         # Decreased\n",
    "    'max_bin': 300,             # Increased\n",
    "    'feature_fraction': 0.85,\n",
    "    'bagging_fraction': 0.85,\n",
    "    'bagging_freq': 7,\n",
    "    'verbose': -1,\n",
    "    'force_row_wise': True,\n",
    "    'random_state': 42,\n",
    "    'extra_trees': True         # Enable extra randomization\n",
    "}\n",
    "\n",
    "print(\"Training ultra-optimized model...\")\n",
    "model_ultra = lgb.train(\n",
    "    params_ultra,\n",
    "    train_data_poly,\n",
    "    valid_sets=[test_data_poly],\n",
    "    num_boost_round=3000,       # Much higher\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(150), # Increased patience\n",
    "        lgb.log_evaluation(300)\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_pred_ultra = model_ultra.predict(X_test_poly, num_iteration=model_ultra.best_iteration)\n",
    "r2_ultra = r2_score(y_test, y_pred_ultra)\n",
    "mae_ultra = mean_absolute_error(y_test, y_pred_ultra)\n",
    "\n",
    "print(f\"Ultra-optimized RÂ² Score: {r2_ultra:.4f}\")\n",
    "print(f\"Ultra-optimized MAE: {mae_ultra:.2f}\")\n",
    "print(f\"Target achieved (RÂ² â‰¥ 0.70): {'YES âœ“' if r2_ultra >= 0.70 else 'NO âœ—'}\")\n",
    "\n",
    "if r2_ultra > best_r2:\n",
    "    best_r2 = r2_ultra\n",
    "    best_model_name = \"Ultra-optimized LightGBM\"\n",
    "    print(f\"New best model: {best_model_name} with RÂ² = {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee05ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final optimized submission...\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Model used: Optimized LightGBM\n",
      "Final RÂ² Score: 0.6505\n",
      "Target RÂ² (â‰¥ 0.70): âœ— NOT ACHIEVED\n",
      "CSV saved as: final_diamond_predictions.csv\n",
      "============================================================\n",
      "\n",
      "Submission preview:\n",
      "      id        price\n",
      "0  20000  1735.985183\n",
      "1  20001  3188.714816\n",
      "2  20002  1097.940952\n",
      "3  20003  1125.660944\n",
      "4  20004  1238.661036\n",
      "\n",
      "Prediction statistics:\n",
      "Min price: $828.39\n",
      "Max price: $14312.27\n",
      "Mean price: $3772.14\n",
      "Median price: $2670.91\n",
      "Total predictions: 30000\n",
      "\n",
      "Using previously saved 'improved_lgbm_model.joblib'\n"
     ]
    }
   ],
   "source": [
    "# Generate final submission with the best performing model\n",
    "print(\"Generating final optimized submission...\")\n",
    "\n",
    "# Prepare enhanced test data\n",
    "X_submission_enhanced = test[feature_columns].copy()\n",
    "\n",
    "# Add polynomial features to submission data\n",
    "for feature in poly_features:\n",
    "    if feature in X_submission_enhanced.columns:\n",
    "        X_submission_enhanced[f'{feature}_squared'] = X_submission_enhanced[feature] ** 2\n",
    "        X_submission_enhanced[f'{feature}_cubed'] = X_submission_enhanced[feature] ** 3\n",
    "\n",
    "# Add interaction features to submission data\n",
    "X_submission_enhanced['carat_volume_interaction'] = X_submission_enhanced['carat'] * X_submission_enhanced['volume']\n",
    "X_submission_enhanced['xyz_product'] = X_submission_enhanced['x'] * X_submission_enhanced['y'] * X_submission_enhanced['z']\n",
    "\n",
    "# Use the best performing model for final predictions\n",
    "if r2_ultra >= 0.70:\n",
    "    final_predictions = model_ultra.predict(X_submission_enhanced, num_iteration=model_ultra.best_iteration)\n",
    "    model_used = \"Ultra-optimized LightGBM\"\n",
    "    final_r2 = r2_ultra\n",
    "else:\n",
    "    # If ultra model doesn't achieve 0.70, use the best available model\n",
    "    final_predictions = model_lgb_optimized.predict(test[feature_columns], num_iteration=model_lgb_optimized.best_iteration)\n",
    "    model_used = \"Optimized LightGBM\"\n",
    "    final_r2 = r2_optimized\n",
    "\n",
    "# Create final submission\n",
    "final_submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "# Ensure prices are positive\n",
    "final_submission['price'] = final_submission['price'].clip(0)\n",
    "\n",
    "# Save the final submission\n",
    "final_submission.to_csv('final_diamond_predictions.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model used: {model_used}\")\n",
    "print(f\"Final RÂ² Score: {final_r2:.4f}\")\n",
    "print(f\"Target RÂ² (â‰¥ 0.70): {'âœ“ ACHIEVED' if final_r2 >= 0.70 else 'âœ— NOT ACHIEVED'}\")\n",
    "print(f\"CSV saved as: final_diamond_predictions.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(final_submission.head())\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"Min price: ${final_submission['price'].min():.2f}\")\n",
    "print(f\"Max price: ${final_submission['price'].max():.2f}\")\n",
    "print(f\"Mean price: ${final_submission['price'].mean():.2f}\")\n",
    "print(f\"Median price: ${final_submission['price'].median():.2f}\")\n",
    "print(f\"Total predictions: {len(final_submission)}\")\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "if r2_ultra >= 0.70:\n",
    "    joblib.dump(model_ultra, 'ultra_optimized_model.joblib')\n",
    "    print(\"\\nUltra-optimized model saved as 'ultra_optimized_model.joblib'\")\n",
    "else:\n",
    "    print(\"\\nUsing previously saved 'improved_lgbm_model.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "367aa909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Creating Anti-Overfitting LightGBM Model\n",
      "============================================================\n",
      "ðŸ“Š Performing 5-fold Cross-Validation...\n",
      "\n",
      "Fold 1/5:\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's rmse: 1389.77\n",
      "  RÂ² Score: 0.6819\n",
      "\n",
      "Fold 2/5:\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid_0's rmse: 1444.87\n",
      "  RÂ² Score: 0.6549\n",
      "\n",
      "Fold 3/5:\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's rmse: 1461.3\n",
      "  RÂ² Score: 0.6613\n",
      "\n",
      "Fold 4/5:\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid_0's rmse: 1445.66\n",
      "  RÂ² Score: 0.6645\n",
      "\n",
      "Fold 5/5:\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[74]\tvalid_0's rmse: 1441.21\n",
      "  RÂ² Score: 0.6813\n",
      "\n",
      "ðŸ“ˆ Cross-Validation Results:\n",
      "  Mean RÂ²: 0.6688 Â± 0.0109\n",
      "  Individual folds: ['0.6819', '0.6549', '0.6613', '0.6645', '0.6813']\n",
      "\n",
      "ðŸ”§ Training Final Conservative Model...\n",
      "  Final model trained with <bound method Booster.num_trees of <lightgbm.basic.Booster object at 0x7176d41b07a0>> trees\n",
      "\n",
      "ðŸŽ² Generating Conservative Predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] The number of features in data (33) is not the same as it was in training data (30).\n",
      "You can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "The number of features in data (33) is not the same as it was in training data (30).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Generate predictions for submission\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸŽ² Generating Conservative Predictions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m conservative_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_conservative\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_submission_enhanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_conservative\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_iteration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Create submission file\u001b[39;00m\n\u001b[1;32m     92\u001b[0m conservative_submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(conservative_predictions)),\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m: conservative_predictions\n\u001b[1;32m     95\u001b[0m })\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py:4767\u001b[0m, in \u001b[0;36mBooster.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features, **kwargs)\u001b[0m\n\u001b[1;32m   4765\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4766\u001b[0m         num_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4767\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_leaf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_contrib\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_contrib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_has_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_has_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4776\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py:1204\u001b[0m, in \u001b[0;36m_InnerPredictor.predict\u001b[0;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, validate_features)\u001b[0m\n\u001b[1;32m   1197\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_csc(\n\u001b[1;32m   1198\u001b[0m         csc\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1199\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[1;32m   1200\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m   1201\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[1;32m   1202\u001b[0m     )\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1204\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pred_for_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data):\n\u001b[1;32m   1211\u001b[0m     preds, nrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pred_for_pyarrow_table(\n\u001b[1;32m   1212\u001b[0m         table\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1213\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[1;32m   1214\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[1;32m   1215\u001b[0m         predict_type\u001b[38;5;241m=\u001b[39mpredict_type,\n\u001b[1;32m   1216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py:1361\u001b[0m, in \u001b[0;36m_InnerPredictor.__pred_for_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds, nrow\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict_np2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iteration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py:1307\u001b[0m, in \u001b[0;36m_InnerPredictor.__inner_predict_np2d\u001b[0;34m(self, mat, start_iteration, num_iteration, predict_type, preds)\u001b[0m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length of pre-allocated predict array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1306\u001b[0m out_num_preds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterPredictForMat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_iteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_c_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_parameter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_num_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_double\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_preds \u001b[38;5;241m!=\u001b[39m out_num_preds\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong length for predict results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightgbm/basic.py:313\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: The number of features in data (33) is not the same as it was in training data (30).\nYou can set ``predict_disable_shape_check=true`` to discard this error, but please be aware what you are doing."
     ]
    }
   ],
   "source": [
    "# === Anti-Overfitting Model for Better Kaggle Performance ===\n",
    "# Goal: Create a model that generalizes better (less gap between validation and Kaggle)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(\"ðŸŽ¯ Creating Anti-Overfitting LightGBM Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Conservative parameters to prevent overfitting\n",
    "params_conservative = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,  # Much smaller (was 300)\n",
    "    'max_depth': 6,    # Smaller depth (was 15)  \n",
    "    'learning_rate': 0.05,  # Higher learning rate (was 0.01)\n",
    "    'feature_fraction': 0.8,  # Add feature sampling\n",
    "    'bagging_fraction': 0.8,  # Add row sampling\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,  # Increase minimum samples\n",
    "    'min_child_weight': 0.001,\n",
    "    'reg_alpha': 1.0,  # L1 regularization\n",
    "    'reg_lambda': 1.0, # L2 regularization\n",
    "    'random_state': 42,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# Use cross-validation for more robust evaluation\n",
    "print(\"ðŸ“Š Performing 5-fold Cross-Validation...\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_r2_scores = []\n",
    "cv_models = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_advanced)):\n",
    "    print(f\"\\nFold {fold + 1}/5:\")\n",
    "    \n",
    "    # Split data\n",
    "    X_fold_train, X_fold_val = X_advanced.iloc[train_idx], X_advanced.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_advanced.iloc[train_idx], y_advanced.iloc[val_idx]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data_fold = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
    "    val_data_fold = lgb.Dataset(X_fold_val, label=y_fold_val, reference=train_data_fold)\n",
    "    \n",
    "    # Train with early stopping\n",
    "    model_fold = lgb.train(\n",
    "        params_conservative,\n",
    "        train_data_fold,\n",
    "        valid_sets=[val_data_fold],\n",
    "        num_boost_round=1000,  # Much fewer rounds (was 8000)\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50),\n",
    "            lgb.log_evaluation(0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    y_pred_fold = model_fold.predict(X_fold_val, num_iteration=model_fold.best_iteration)\n",
    "    r2_fold = r2_score(y_fold_val, y_pred_fold)\n",
    "    \n",
    "    cv_r2_scores.append(r2_fold)\n",
    "    cv_models.append(model_fold)\n",
    "    \n",
    "    print(f\"  RÂ² Score: {r2_fold:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Cross-Validation Results:\")\n",
    "print(f\"  Mean RÂ²: {np.mean(cv_r2_scores):.4f} Â± {np.std(cv_r2_scores):.4f}\")\n",
    "print(f\"  Individual folds: {[f'{score:.4f}' for score in cv_r2_scores]}\")\n",
    "\n",
    "# Train final model on all data with conservative parameters\n",
    "print(f\"\\nðŸ”§ Training Final Conservative Model...\")\n",
    "train_data_final = lgb.Dataset(X_advanced, label=y_advanced)\n",
    "\n",
    "model_conservative = lgb.train(\n",
    "    params_conservative,\n",
    "    train_data_final,\n",
    "    num_boost_round=int(np.mean([model.best_iteration for model in cv_models])),\n",
    "    callbacks=[lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "print(f\"  Final model trained with {model_conservative.num_trees} trees\")\n",
    "\n",
    "# Generate predictions for submission\n",
    "print(f\"\\nðŸŽ² Generating Conservative Predictions...\")\n",
    "conservative_predictions = model_conservative.predict(X_submission_enhanced, num_iteration=model_conservative.best_iteration)\n",
    "\n",
    "# Create submission file\n",
    "conservative_submission = pd.DataFrame({\n",
    "    'id': range(len(conservative_predictions)),\n",
    "    'price': conservative_predictions\n",
    "})\n",
    "\n",
    "# Save conservative model and predictions\n",
    "conservative_filename = f\"diamond_conservative_cv_{np.mean(cv_r2_scores):.4f}.csv\"\n",
    "conservative_model_file = f\"conservative_lgb_cv_{np.mean(cv_r2_scores):.4f}.joblib\"\n",
    "\n",
    "conservative_submission.to_csv(conservative_filename, index=False)\n",
    "joblib.dump(model_conservative, conservative_model_file)\n",
    "\n",
    "print(f\"âœ… Conservative Model Results:\")\n",
    "print(f\"  Cross-validation RÂ²: {np.mean(cv_r2_scores):.4f} Â± {np.std(cv_r2_scores):.4f}\")\n",
    "print(f\"  Submission saved: {conservative_filename}\")\n",
    "print(f\"  Model saved: {conservative_model_file}\")\n",
    "print(f\"  This model should generalize better to Kaggle!\")\n",
    "\n",
    "# Compare with previous ultra model\n",
    "print(f\"\\nâš–ï¸ Model Comparison:\")\n",
    "print(f\"  Ultra Model (validation): {r2_ultra_lgb:.4f} â†’ Kaggle: 0.6188 (Gap: {r2_ultra_lgb - 0.6188:.4f})\")\n",
    "print(f\"  Conservative Model (CV): {np.mean(cv_r2_scores):.4f} â†’ Expected Kaggle: ~{np.mean(cv_r2_scores):.4f}\")\n",
    "print(f\"  Expected improvement in Kaggle score: {np.mean(cv_r2_scores) - 0.6188:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fefd49b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Feature Mismatch Debug:\n",
      "X_advanced shape: (18239, 30)\n",
      "X_submission_enhanced shape: (30000, 33)\n",
      "X_advanced columns: 30\n",
      "X_submission_enhanced columns: 33\n",
      "\n",
      "X_advanced columns:\n",
      "  1. carat\n",
      "  2. cut_encoded\n",
      "  3. color_encoded\n",
      "  4. clarity_encoded\n",
      "  5. depth\n",
      "  6. table\n",
      "  7. x\n",
      "  8. y\n",
      "  9. z\n",
      "  10. volume\n",
      "  11. surface_area\n",
      "  12. carat_per_volume\n",
      "  13. density\n",
      "  14. aspect_xy\n",
      "  15. aspect_xz\n",
      "  16. cut_quality\n",
      "  17. color_quality\n",
      "  18. clarity_quality\n",
      "  19. overall_quality\n",
      "  20. log_carat\n",
      "  21. log_volume\n",
      "  22. carat_squared\n",
      "  23. volume_squared\n",
      "  24. overall_quality_squared\n",
      "  25. carat_cubed\n",
      "  26. volume_cubed\n",
      "  27. overall_quality_cubed\n",
      "  28. carat_quality\n",
      "  29. volume_quality\n",
      "  30. carat_bin\n",
      "\n",
      "X_submission_enhanced columns:\n",
      "  1. carat\n",
      "  2. cut_encoded\n",
      "  3. color_encoded\n",
      "  4. clarity_encoded\n",
      "  5. depth\n",
      "  6. table\n",
      "  7. x\n",
      "  8. y\n",
      "  9. z\n",
      "  10. volume\n",
      "  11. carat_per_volume\n",
      "  12. surface_area\n",
      "  13. aspect_ratio_xy\n",
      "  14. aspect_ratio_xz\n",
      "  15. depth_ratio\n",
      "  16. table_ratio\n",
      "  17. carat_cut_interaction\n",
      "  18. carat_color_interaction\n",
      "  19. carat_clarity_interaction\n",
      "  20. quality_score\n",
      "  21. density\n",
      "  22. carat_squared\n",
      "  23. carat_cubed\n",
      "  24. x_squared\n",
      "  25. x_cubed\n",
      "  26. y_squared\n",
      "  27. y_cubed\n",
      "  28. z_squared\n",
      "  29. z_cubed\n",
      "  30. volume_squared\n",
      "  31. volume_cubed\n",
      "  32. carat_volume_interaction\n",
      "  33. xyz_product\n",
      "\n",
      "Missing in submission: {'log_carat', 'overall_quality_cubed', 'carat_bin', 'carat_quality', 'aspect_xz', 'overall_quality', 'color_quality', 'overall_quality_squared', 'cut_quality', 'clarity_quality', 'log_volume', 'volume_quality', 'aspect_xy'}\n",
      "Extra in submission: {'quality_score', 'aspect_ratio_xy', 'x_cubed', 'carat_color_interaction', 'x_squared', 'z_cubed', 'y_squared', 'y_cubed', 'xyz_product', 'carat_cut_interaction', 'carat_volume_interaction', 'depth_ratio', 'table_ratio', 'carat_clarity_interaction', 'aspect_ratio_xz', 'z_squared'}\n",
      "\n",
      "Common features: 17\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['aspect_xy', 'aspect_xz', 'cut_quality', 'color_quality', 'clarity_quality', 'overall_quality', 'log_carat', 'log_volume', 'overall_quality_squared', 'overall_quality_cubed', 'carat_quality', 'volume_quality', 'carat_bin'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCommon features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(common_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure same feature order\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m X_submission_aligned \u001b[38;5;241m=\u001b[39m \u001b[43mX_submission_enhanced\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_advanced\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAligned submission shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_submission_aligned\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['aspect_xy', 'aspect_xz', 'cut_quality', 'color_quality', 'clarity_quality', 'overall_quality', 'log_carat', 'log_volume', 'overall_quality_squared', 'overall_quality_cubed', 'carat_quality', 'volume_quality', 'carat_bin'] not in index\""
     ]
    }
   ],
   "source": [
    "# Debug feature mismatch\n",
    "print(\"ðŸ” Feature Mismatch Debug:\")\n",
    "print(f\"X_advanced shape: {X_advanced.shape}\")\n",
    "print(f\"X_submission_enhanced shape: {X_submission_enhanced.shape}\")\n",
    "print(f\"X_advanced columns: {len(X_advanced.columns)}\")\n",
    "print(f\"X_submission_enhanced columns: {len(X_submission_enhanced.columns)}\")\n",
    "\n",
    "print(f\"\\nX_advanced columns:\")\n",
    "for i, col in enumerate(X_advanced.columns):\n",
    "    print(f\"  {i+1}. {col}\")\n",
    "\n",
    "print(f\"\\nX_submission_enhanced columns:\")\n",
    "for i, col in enumerate(X_submission_enhanced.columns):\n",
    "    print(f\"  {i+1}. {col}\")\n",
    "\n",
    "# Check which columns are missing\n",
    "missing_in_submission = set(X_advanced.columns) - set(X_submission_enhanced.columns)\n",
    "extra_in_submission = set(X_submission_enhanced.columns) - set(X_advanced.columns)\n",
    "\n",
    "print(f\"\\nMissing in submission: {missing_in_submission}\")\n",
    "print(f\"Extra in submission: {extra_in_submission}\")\n",
    "\n",
    "# Use the same features for both\n",
    "common_features = list(set(X_advanced.columns) & set(X_submission_enhanced.columns))\n",
    "print(f\"\\nCommon features: {len(common_features)}\")\n",
    "\n",
    "# Ensure same feature order\n",
    "X_submission_aligned = X_submission_enhanced[X_advanced.columns]\n",
    "print(f\"Aligned submission shape: {X_submission_aligned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1df6d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Recreating submission features to match training data...\n",
      "Applying preprocessing...\n",
      "Creating engineered features...\n",
      "âœ… Fixed submission shape: (30000, 30)\n",
      "Training shape: (18239, 30)\n",
      "Features match: True\n",
      "\n",
      "ðŸŽ² Generating Conservative Predictions with Fixed Features...\n",
      "\n",
      "âœ… Conservative Model Completed:\n",
      "  Cross-validation RÂ²: 0.6688 Â± 0.0109\n",
      "  Submission saved: diamond_conservative_r2_0.6688.csv\n",
      "  Model saved: conservative_lgb_r2_0.6688.joblib\n",
      "  Features: 30\n",
      "\n",
      "âš–ï¸ Expected Performance:\n",
      "  Ultra Model: Validation 0.6603 â†’ Kaggle 0.6188 (Gap: 0.0415)\n",
      "  Conservative: CV 0.6688 â†’ Expected Kaggle ~0.6688\n",
      "  Predicted Kaggle improvement: 0.0500\n",
      "\n",
      "ðŸŽ¯ Key Model Differences:\n",
      "  Ultra: 300 leaves, 15 depth, 0.01 lr, 8000 rounds\n",
      "  Conservative: 31 leaves, 6 depth, 0.05 lr, ~66 rounds\n",
      "  Conservative adds: feature/bagging sampling + L1/L2 regularization\n"
     ]
    }
   ],
   "source": [
    "# === Fix Submission Features & Retry Conservative Model ===\n",
    "print(\"ðŸ”§ Recreating submission features to match training data...\")\n",
    "\n",
    "# Start with the basic submission data\n",
    "X_submission_fixed = test.copy()\n",
    "\n",
    "# Apply same preprocessing as training data\n",
    "print(\"Applying preprocessing...\")\n",
    "\n",
    "# Encode categorical variables\n",
    "X_submission_fixed['cut_encoded'] = le_cut.transform(X_submission_fixed['cut'])\n",
    "X_submission_fixed['color_encoded'] = le_color.transform(X_submission_fixed['color']) \n",
    "X_submission_fixed['clarity_encoded'] = le_clarity.transform(X_submission_fixed['clarity'])\n",
    "\n",
    "# Feature engineering - same as training\n",
    "print(\"Creating engineered features...\")\n",
    "\n",
    "# Basic engineered features\n",
    "X_submission_fixed['volume'] = X_submission_fixed['x'] * X_submission_fixed['y'] * X_submission_fixed['z']\n",
    "X_submission_fixed['surface_area'] = 2 * (X_submission_fixed['x']*X_submission_fixed['y'] + \n",
    "                                         X_submission_fixed['x']*X_submission_fixed['z'] + \n",
    "                                         X_submission_fixed['y']*X_submission_fixed['z'])\n",
    "X_submission_fixed['carat_per_volume'] = X_submission_fixed['carat'] / (X_submission_fixed['volume'] + 1e-8)\n",
    "X_submission_fixed['density'] = X_submission_fixed['carat'] / (X_submission_fixed['volume'] + 1e-8)\n",
    "X_submission_fixed['aspect_xy'] = X_submission_fixed['x'] / (X_submission_fixed['y'] + 1e-8)\n",
    "X_submission_fixed['aspect_xz'] = X_submission_fixed['x'] / (X_submission_fixed['z'] + 1e-8)\n",
    "\n",
    "# Quality features\n",
    "X_submission_fixed['cut_quality'] = X_submission_fixed['cut'].map(cut_quality)\n",
    "X_submission_fixed['color_quality'] = X_submission_fixed['color'].map(color_quality)\n",
    "X_submission_fixed['clarity_quality'] = X_submission_fixed['clarity'].map(clarity_quality)\n",
    "X_submission_fixed['overall_quality'] = (X_submission_fixed['cut_quality'] + \n",
    "                                        X_submission_fixed['color_quality'] + \n",
    "                                        X_submission_fixed['clarity_quality']) / 3\n",
    "\n",
    "# Log features\n",
    "X_submission_fixed['log_carat'] = np.log1p(X_submission_fixed['carat'])\n",
    "X_submission_fixed['log_volume'] = np.log1p(X_submission_fixed['volume'])\n",
    "\n",
    "# Polynomial features\n",
    "X_submission_fixed['carat_squared'] = X_submission_fixed['carat'] ** 2\n",
    "X_submission_fixed['volume_squared'] = X_submission_fixed['volume'] ** 2\n",
    "X_submission_fixed['overall_quality_squared'] = X_submission_fixed['overall_quality'] ** 2\n",
    "X_submission_fixed['carat_cubed'] = X_submission_fixed['carat'] ** 3\n",
    "X_submission_fixed['volume_cubed'] = X_submission_fixed['volume'] ** 3\n",
    "X_submission_fixed['overall_quality_cubed'] = X_submission_fixed['overall_quality'] ** 3\n",
    "\n",
    "# Interaction features\n",
    "X_submission_fixed['carat_quality'] = X_submission_fixed['carat'] * X_submission_fixed['overall_quality']\n",
    "X_submission_fixed['volume_quality'] = X_submission_fixed['volume'] * X_submission_fixed['overall_quality']\n",
    "\n",
    "# Binning feature\n",
    "X_submission_fixed['carat_bin'] = pd.cut(X_submission_fixed['carat'], bins=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "le_bin = LabelEncoder()\n",
    "X_submission_fixed['carat_bin'] = le_bin.fit_transform(X_submission_fixed['carat_bin'].astype(str))\n",
    "\n",
    "# Select same features as training\n",
    "feature_columns_fixed = X_advanced.columns.tolist()\n",
    "X_submission_final = X_submission_fixed[feature_columns_fixed]\n",
    "\n",
    "print(f\"âœ… Fixed submission shape: {X_submission_final.shape}\")\n",
    "print(f\"Training shape: {X_advanced.shape}\")\n",
    "print(f\"Features match: {list(X_submission_final.columns) == list(X_advanced.columns)}\")\n",
    "\n",
    "# Now generate conservative predictions with corrected features\n",
    "print(f\"\\nðŸŽ² Generating Conservative Predictions with Fixed Features...\")\n",
    "conservative_predictions = model_conservative.predict(X_submission_final, num_iteration=model_conservative.best_iteration)\n",
    "\n",
    "# Create submission file\n",
    "conservative_submission = pd.DataFrame({\n",
    "    'id': range(len(conservative_predictions)),\n",
    "    'price': conservative_predictions\n",
    "})\n",
    "\n",
    "# Save conservative model and predictions\n",
    "cv_score = np.mean(cv_r2_scores)\n",
    "conservative_filename = f\"diamond_conservative_r2_{cv_score:.4f}.csv\"\n",
    "conservative_model_file = f\"conservative_lgb_r2_{cv_score:.4f}.joblib\"\n",
    "\n",
    "conservative_submission.to_csv(conservative_filename, index=False)\n",
    "joblib.dump(model_conservative, conservative_model_file)\n",
    "\n",
    "print(f\"\\nâœ… Conservative Model Completed:\")\n",
    "print(f\"  Cross-validation RÂ²: {cv_score:.4f} Â± {np.std(cv_r2_scores):.4f}\")\n",
    "print(f\"  Submission saved: {conservative_filename}\")\n",
    "print(f\"  Model saved: {conservative_model_file}\")\n",
    "print(f\"  Features: {len(feature_columns_fixed)}\")\n",
    "\n",
    "print(f\"\\nâš–ï¸ Expected Performance:\")\n",
    "print(f\"  Ultra Model: Validation {r2_ultra_lgb:.4f} â†’ Kaggle 0.6188 (Gap: {r2_ultra_lgb - 0.6188:.4f})\")\n",
    "print(f\"  Conservative: CV {cv_score:.4f} â†’ Expected Kaggle ~{cv_score:.4f}\")\n",
    "print(f\"  Predicted Kaggle improvement: {cv_score - 0.6188:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key Model Differences:\")\n",
    "print(f\"  Ultra: 300 leaves, 15 depth, 0.01 lr, 8000 rounds\")\n",
    "print(f\"  Conservative: 31 leaves, 6 depth, 0.05 lr, ~{int(np.mean([model.best_iteration for model in cv_models]))} rounds\")\n",
    "print(f\"  Conservative adds: feature/bagging sampling + L1/L2 regularization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
